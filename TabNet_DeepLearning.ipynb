{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dreamquark-ai/tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc017ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f5751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"dataset.tsv\", sep=\"\\t\")\n",
    "LabelNames = Data.columns[56:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae949e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = Data.loc[:, SelectedCols], Data.loc[:, LabelNames]\n",
    "X, Y = Data.iloc[:, 1:56], Data.loc[:, LabelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dda29c0e-7f5e-45a8-b08b-099b5d311fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Female\n",
       "1       Female\n",
       "2       Female\n",
       "3       Female\n",
       "4       Female\n",
       "         ...  \n",
       "4639    Female\n",
       "4640      Male\n",
       "4641    Female\n",
       "4642      Male\n",
       "4643      Male\n",
       "Name: Sex.Male.1..Female.2., Length: 4644, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc237af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302da1c",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "## Fine-tunning the Pretrained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ac3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosal/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_tabnet\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "nCategory = [len(X_train.iloc[:,idx].unique()) for idx, col in enumerate(X_train.columns)]\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    cat_idxs=[idx for idx, nCat in enumerate(nCategory) if nCat <= 2],\n",
    "    cat_dims=[nCat for idx, nCat in enumerate(nCategory) if nCat <= 2],\n",
    "    cat_emb_dim=5,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),\n",
    "    mask_type='entmax', # \"sparsemax\",\n",
    "    n_shared_decoder=1, # nb shared glu for decoding\n",
    "    n_indep_decoder=1, # nb independent glu for decoding\n",
    "    # grouped_features=[[0, 1]], # you can group features together here\n",
    "    verbose=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42131593",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_model.load_model('./MODEL/PretrainedModel_0.3788.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff23fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 10076.23709| val_0_unsup_loss_numpy: 28.58395004272461|  0:00:01s\n",
      "epoch 5  | loss: 2.20777 | val_0_unsup_loss_numpy: 1.53056001663208|  0:00:03s\n",
      "epoch 10 | loss: 1.14497 | val_0_unsup_loss_numpy: 1.1392500400543213|  0:00:05s\n",
      "epoch 15 | loss: 1.10751 | val_0_unsup_loss_numpy: 1.0642499923706055|  0:00:06s\n",
      "epoch 20 | loss: 1.07711 | val_0_unsup_loss_numpy: 1.024049997329712|  0:00:08s\n",
      "epoch 25 | loss: 1.03102 | val_0_unsup_loss_numpy: 0.9632899761199951|  0:00:10s\n",
      "epoch 30 | loss: 0.94618 | val_0_unsup_loss_numpy: 0.8659800291061401|  0:00:12s\n",
      "epoch 35 | loss: 0.87737 | val_0_unsup_loss_numpy: 0.7919399738311768|  0:00:14s\n",
      "epoch 40 | loss: 0.84995 | val_0_unsup_loss_numpy: 0.7683899998664856|  0:00:16s\n",
      "epoch 45 | loss: 0.80579 | val_0_unsup_loss_numpy: 0.7403200268745422|  0:00:17s\n",
      "epoch 50 | loss: 0.81125 | val_0_unsup_loss_numpy: 0.7226899862289429|  0:00:19s\n",
      "epoch 55 | loss: 0.78451 | val_0_unsup_loss_numpy: 0.7099300026893616|  0:00:21s\n",
      "epoch 60 | loss: 0.77585 | val_0_unsup_loss_numpy: 0.6914899945259094|  0:00:23s\n",
      "epoch 65 | loss: 0.76055 | val_0_unsup_loss_numpy: 0.6767899990081787|  0:00:25s\n",
      "epoch 70 | loss: 0.7623  | val_0_unsup_loss_numpy: 0.6594399809837341|  0:00:27s\n",
      "epoch 75 | loss: 0.7308  | val_0_unsup_loss_numpy: 0.6354399919509888|  0:00:29s\n",
      "epoch 80 | loss: 0.71638 | val_0_unsup_loss_numpy: 0.6111099720001221|  0:00:31s\n",
      "epoch 85 | loss: 0.70892 | val_0_unsup_loss_numpy: 0.6016700267791748|  0:00:33s\n",
      "epoch 90 | loss: 0.68918 | val_0_unsup_loss_numpy: 0.5916500091552734|  0:00:34s\n",
      "epoch 95 | loss: 0.6866  | val_0_unsup_loss_numpy: 0.5786399841308594|  0:00:36s\n",
      "epoch 100| loss: 0.68413 | val_0_unsup_loss_numpy: 0.5735399723052979|  0:00:38s\n",
      "epoch 105| loss: 0.66166 | val_0_unsup_loss_numpy: 0.5672900080680847|  0:00:40s\n",
      "epoch 110| loss: 0.68041 | val_0_unsup_loss_numpy: 0.5631999969482422|  0:00:42s\n",
      "epoch 115| loss: 0.66097 | val_0_unsup_loss_numpy: 0.5448099970817566|  0:00:44s\n",
      "epoch 120| loss: 0.66845 | val_0_unsup_loss_numpy: 0.539359986782074|  0:00:46s\n",
      "epoch 125| loss: 0.64934 | val_0_unsup_loss_numpy: 0.5330700278282166|  0:00:48s\n",
      "epoch 130| loss: 0.65878 | val_0_unsup_loss_numpy: 0.5281800031661987|  0:00:49s\n",
      "epoch 135| loss: 0.65064 | val_0_unsup_loss_numpy: 0.5290799736976624|  0:00:51s\n",
      "epoch 140| loss: 0.64078 | val_0_unsup_loss_numpy: 0.524940013885498|  0:00:53s\n",
      "epoch 145| loss: 0.64331 | val_0_unsup_loss_numpy: 0.5124199986457825|  0:00:55s\n",
      "epoch 150| loss: 0.63884 | val_0_unsup_loss_numpy: 0.512939989566803|  0:00:57s\n",
      "epoch 155| loss: 0.63738 | val_0_unsup_loss_numpy: 0.4966599941253662|  0:00:59s\n",
      "epoch 160| loss: 0.628   | val_0_unsup_loss_numpy: 0.49612000584602356|  0:01:01s\n",
      "epoch 165| loss: 0.61071 | val_0_unsup_loss_numpy: 0.479449987411499|  0:01:02s\n",
      "epoch 170| loss: 0.62192 | val_0_unsup_loss_numpy: 0.4799099862575531|  0:01:04s\n",
      "epoch 175| loss: 0.62161 | val_0_unsup_loss_numpy: 0.48778998851776123|  0:01:06s\n",
      "epoch 180| loss: 0.62986 | val_0_unsup_loss_numpy: 0.4704900085926056|  0:01:08s\n",
      "epoch 185| loss: 0.62372 | val_0_unsup_loss_numpy: 0.47753000259399414|  0:01:09s\n",
      "epoch 190| loss: 0.60524 | val_0_unsup_loss_numpy: 0.475600004196167|  0:01:11s\n",
      "epoch 195| loss: 0.60562 | val_0_unsup_loss_numpy: 0.4745500087738037|  0:01:13s\n",
      "epoch 200| loss: 0.59913 | val_0_unsup_loss_numpy: 0.47255000472068787|  0:01:15s\n",
      "epoch 205| loss: 0.6217  | val_0_unsup_loss_numpy: 0.46775999665260315|  0:01:17s\n",
      "epoch 210| loss: 0.59657 | val_0_unsup_loss_numpy: 0.46990999579429626|  0:01:19s\n",
      "epoch 215| loss: 0.60435 | val_0_unsup_loss_numpy: 0.47593000531196594|  0:01:21s\n",
      "epoch 220| loss: 0.61409 | val_0_unsup_loss_numpy: 0.4711199998855591|  0:01:22s\n",
      "epoch 225| loss: 0.6029  | val_0_unsup_loss_numpy: 0.45596998929977417|  0:01:24s\n",
      "epoch 230| loss: 0.59109 | val_0_unsup_loss_numpy: 0.4491499960422516|  0:01:26s\n",
      "epoch 235| loss: 0.61104 | val_0_unsup_loss_numpy: 0.44624999165534973|  0:01:28s\n",
      "epoch 240| loss: 0.57597 | val_0_unsup_loss_numpy: 0.44765999913215637|  0:01:30s\n",
      "epoch 245| loss: 0.58622 | val_0_unsup_loss_numpy: 0.4338200092315674|  0:01:32s\n",
      "epoch 250| loss: 0.56554 | val_0_unsup_loss_numpy: 0.4388499855995178|  0:01:34s\n",
      "epoch 255| loss: 0.59423 | val_0_unsup_loss_numpy: 0.4474700093269348|  0:01:35s\n",
      "epoch 260| loss: 0.5784  | val_0_unsup_loss_numpy: 0.43130001425743103|  0:01:37s\n",
      "epoch 265| loss: 0.57814 | val_0_unsup_loss_numpy: 0.45583000779151917|  0:01:39s\n",
      "epoch 270| loss: 0.57551 | val_0_unsup_loss_numpy: 0.420879989862442|  0:01:41s\n",
      "epoch 275| loss: 0.56775 | val_0_unsup_loss_numpy: 0.41986000537872314|  0:01:43s\n",
      "epoch 280| loss: 0.58123 | val_0_unsup_loss_numpy: 0.42998000979423523|  0:01:45s\n",
      "epoch 285| loss: 0.58848 | val_0_unsup_loss_numpy: 0.43007999658584595|  0:01:46s\n",
      "epoch 290| loss: 0.57572 | val_0_unsup_loss_numpy: 0.4203299880027771|  0:01:48s\n",
      "epoch 295| loss: 0.56678 | val_0_unsup_loss_numpy: 0.41370001435279846|  0:01:50s\n",
      "epoch 300| loss: 0.57803 | val_0_unsup_loss_numpy: 0.4221700131893158|  0:01:52s\n",
      "epoch 305| loss: 0.56999 | val_0_unsup_loss_numpy: 0.4171600043773651|  0:01:54s\n",
      "epoch 310| loss: 0.57603 | val_0_unsup_loss_numpy: 0.42414000630378723|  0:01:56s\n",
      "epoch 315| loss: 0.57197 | val_0_unsup_loss_numpy: 0.4273200035095215|  0:01:58s\n",
      "epoch 320| loss: 0.57406 | val_0_unsup_loss_numpy: 0.41600000858306885|  0:01:59s\n",
      "epoch 325| loss: 0.56097 | val_0_unsup_loss_numpy: 0.41525998711586|  0:02:01s\n",
      "epoch 330| loss: 0.56341 | val_0_unsup_loss_numpy: 0.4151099920272827|  0:02:03s\n",
      "epoch 335| loss: 0.56804 | val_0_unsup_loss_numpy: 0.41971999406814575|  0:02:05s\n",
      "epoch 340| loss: 0.57809 | val_0_unsup_loss_numpy: 0.4171000123023987|  0:02:07s\n",
      "epoch 345| loss: 0.57721 | val_0_unsup_loss_numpy: 0.4197700023651123|  0:02:08s\n",
      "epoch 350| loss: 0.57483 | val_0_unsup_loss_numpy: 0.4163599908351898|  0:02:10s\n",
      "epoch 355| loss: 0.57931 | val_0_unsup_loss_numpy: 0.4197100102901459|  0:02:12s\n",
      "epoch 360| loss: 0.57431 | val_0_unsup_loss_numpy: 0.4214699864387512|  0:02:14s\n",
      "epoch 365| loss: 0.57249 | val_0_unsup_loss_numpy: 0.4162299931049347|  0:02:16s\n",
      "epoch 370| loss: 0.56125 | val_0_unsup_loss_numpy: 0.41508999466896057|  0:02:18s\n",
      "epoch 375| loss: 0.57063 | val_0_unsup_loss_numpy: 0.4083400070667267|  0:02:20s\n",
      "epoch 380| loss: 0.55842 | val_0_unsup_loss_numpy: 0.4113200008869171|  0:02:21s\n",
      "epoch 385| loss: 0.56279 | val_0_unsup_loss_numpy: 0.40490999817848206|  0:02:23s\n",
      "epoch 390| loss: 0.56192 | val_0_unsup_loss_numpy: 0.41815000772476196|  0:02:25s\n",
      "epoch 395| loss: 0.57281 | val_0_unsup_loss_numpy: 0.41093000769615173|  0:02:27s\n",
      "epoch 400| loss: 0.55254 | val_0_unsup_loss_numpy: 0.41001999378204346|  0:02:29s\n",
      "epoch 405| loss: 0.57154 | val_0_unsup_loss_numpy: 0.41150999069213867|  0:02:31s\n",
      "epoch 410| loss: 0.55443 | val_0_unsup_loss_numpy: 0.40702998638153076|  0:02:33s\n",
      "epoch 415| loss: 0.54512 | val_0_unsup_loss_numpy: 0.4077500104904175|  0:02:34s\n",
      "epoch 420| loss: 0.57839 | val_0_unsup_loss_numpy: 0.40242999792099|  0:02:36s\n",
      "epoch 425| loss: 0.56884 | val_0_unsup_loss_numpy: 0.4111500084400177|  0:02:38s\n",
      "epoch 430| loss: 0.57203 | val_0_unsup_loss_numpy: 0.39866000413894653|  0:02:40s\n",
      "epoch 435| loss: 0.56305 | val_0_unsup_loss_numpy: 0.4038099944591522|  0:02:41s\n",
      "epoch 440| loss: 0.54567 | val_0_unsup_loss_numpy: 0.40808001160621643|  0:02:43s\n",
      "epoch 445| loss: 0.56527 | val_0_unsup_loss_numpy: 0.40751999616622925|  0:02:45s\n",
      "epoch 450| loss: 0.55496 | val_0_unsup_loss_numpy: 0.40380001068115234|  0:02:47s\n",
      "epoch 455| loss: 0.56178 | val_0_unsup_loss_numpy: 0.39902999997138977|  0:02:49s\n",
      "epoch 460| loss: 0.56213 | val_0_unsup_loss_numpy: 0.39921998977661133|  0:02:51s\n",
      "epoch 465| loss: 0.57147 | val_0_unsup_loss_numpy: 0.4107300043106079|  0:02:53s\n",
      "epoch 470| loss: 0.55661 | val_0_unsup_loss_numpy: 0.40174001455307007|  0:02:54s\n",
      "epoch 475| loss: 0.55701 | val_0_unsup_loss_numpy: 0.405239999294281|  0:02:56s\n",
      "epoch 480| loss: 0.54549 | val_0_unsup_loss_numpy: 0.3948499858379364|  0:02:58s\n",
      "epoch 485| loss: 0.55959 | val_0_unsup_loss_numpy: 0.3995699882507324|  0:03:00s\n",
      "epoch 490| loss: 0.54079 | val_0_unsup_loss_numpy: 0.3939000070095062|  0:03:02s\n",
      "epoch 495| loss: 0.54884 | val_0_unsup_loss_numpy: 0.39844998717308044|  0:03:03s\n",
      "epoch 500| loss: 0.55783 | val_0_unsup_loss_numpy: 0.3990199863910675|  0:03:05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 505| loss: 0.5565  | val_0_unsup_loss_numpy: 0.3974800109863281|  0:03:07s\n",
      "epoch 510| loss: 0.55386 | val_0_unsup_loss_numpy: 0.3972199857234955|  0:03:09s\n",
      "epoch 515| loss: 0.54283 | val_0_unsup_loss_numpy: 0.39548999071121216|  0:03:11s\n",
      "epoch 520| loss: 0.5343  | val_0_unsup_loss_numpy: 0.4000000059604645|  0:03:12s\n",
      "epoch 525| loss: 0.5531  | val_0_unsup_loss_numpy: 0.39478999376296997|  0:03:14s\n",
      "epoch 530| loss: 0.55051 | val_0_unsup_loss_numpy: 0.3981100022792816|  0:03:16s\n",
      "epoch 535| loss: 0.55632 | val_0_unsup_loss_numpy: 0.3954800069332123|  0:03:18s\n",
      "epoch 540| loss: 0.56735 | val_0_unsup_loss_numpy: 0.3996100127696991|  0:03:20s\n",
      "epoch 545| loss: 0.56326 | val_0_unsup_loss_numpy: 0.40084001421928406|  0:03:22s\n",
      "epoch 550| loss: 0.54587 | val_0_unsup_loss_numpy: 0.39831000566482544|  0:03:23s\n",
      "epoch 555| loss: 0.54907 | val_0_unsup_loss_numpy: 0.3943299949169159|  0:03:25s\n",
      "epoch 560| loss: 0.54799 | val_0_unsup_loss_numpy: 0.39438000321388245|  0:03:27s\n",
      "epoch 565| loss: 0.53212 | val_0_unsup_loss_numpy: 0.3919300138950348|  0:03:29s\n",
      "epoch 570| loss: 0.55518 | val_0_unsup_loss_numpy: 0.40217000246047974|  0:03:31s\n",
      "epoch 575| loss: 0.54346 | val_0_unsup_loss_numpy: 0.3899799883365631|  0:03:33s\n",
      "epoch 580| loss: 0.5327  | val_0_unsup_loss_numpy: 0.39552000164985657|  0:03:35s\n",
      "epoch 585| loss: 0.54312 | val_0_unsup_loss_numpy: 0.3949899971485138|  0:03:37s\n",
      "epoch 590| loss: 0.54174 | val_0_unsup_loss_numpy: 0.3961000144481659|  0:03:39s\n",
      "epoch 595| loss: 0.55704 | val_0_unsup_loss_numpy: 0.3985300064086914|  0:03:41s\n",
      "epoch 600| loss: 0.53987 | val_0_unsup_loss_numpy: 0.3908500075340271|  0:03:42s\n",
      "epoch 605| loss: 0.54928 | val_0_unsup_loss_numpy: 0.392769992351532|  0:03:44s\n",
      "epoch 610| loss: 0.5564  | val_0_unsup_loss_numpy: 0.4089199900627136|  0:03:46s\n",
      "epoch 615| loss: 0.54735 | val_0_unsup_loss_numpy: 0.3879599869251251|  0:03:48s\n",
      "epoch 620| loss: 0.54194 | val_0_unsup_loss_numpy: 0.3922800123691559|  0:03:50s\n",
      "epoch 625| loss: 0.53656 | val_0_unsup_loss_numpy: 0.3970400094985962|  0:03:52s\n",
      "epoch 630| loss: 0.54135 | val_0_unsup_loss_numpy: 0.39177998900413513|  0:03:54s\n",
      "epoch 635| loss: 0.53908 | val_0_unsup_loss_numpy: 0.3950499892234802|  0:03:56s\n",
      "epoch 640| loss: 0.5614  | val_0_unsup_loss_numpy: 0.3919999897480011|  0:03:58s\n",
      "epoch 645| loss: 0.55797 | val_0_unsup_loss_numpy: 0.3960599899291992|  0:04:00s\n",
      "epoch 650| loss: 0.54138 | val_0_unsup_loss_numpy: 0.39056000113487244|  0:04:01s\n",
      "epoch 655| loss: 0.53285 | val_0_unsup_loss_numpy: 0.3959999978542328|  0:04:03s\n",
      "epoch 660| loss: 0.54402 | val_0_unsup_loss_numpy: 0.390390008687973|  0:04:05s\n",
      "epoch 665| loss: 0.54442 | val_0_unsup_loss_numpy: 0.394540011882782|  0:04:07s\n",
      "epoch 670| loss: 0.52268 | val_0_unsup_loss_numpy: 0.393779993057251|  0:04:09s\n",
      "epoch 675| loss: 0.54426 | val_0_unsup_loss_numpy: 0.39458999037742615|  0:04:11s\n",
      "epoch 680| loss: 0.54912 | val_0_unsup_loss_numpy: 0.3939700126647949|  0:04:12s\n",
      "epoch 685| loss: 0.54194 | val_0_unsup_loss_numpy: 0.4141499996185303|  0:04:14s\n",
      "epoch 690| loss: 0.54167 | val_0_unsup_loss_numpy: 0.39890000224113464|  0:04:16s\n",
      "epoch 695| loss: 0.5401  | val_0_unsup_loss_numpy: 0.3872300088405609|  0:04:18s\n",
      "epoch 700| loss: 0.54689 | val_0_unsup_loss_numpy: 0.3909299969673157|  0:04:20s\n",
      "epoch 705| loss: 0.54507 | val_0_unsup_loss_numpy: 0.3889800012111664|  0:04:22s\n",
      "epoch 710| loss: 0.54088 | val_0_unsup_loss_numpy: 0.39239001274108887|  0:04:24s\n",
      "epoch 715| loss: 0.5373  | val_0_unsup_loss_numpy: 0.3861500024795532|  0:04:26s\n",
      "epoch 720| loss: 0.53941 | val_0_unsup_loss_numpy: 0.38545000553131104|  0:04:28s\n",
      "epoch 725| loss: 0.54282 | val_0_unsup_loss_numpy: 0.39142000675201416|  0:04:30s\n",
      "epoch 730| loss: 0.54679 | val_0_unsup_loss_numpy: 0.3881300091743469|  0:04:31s\n",
      "epoch 735| loss: 0.54726 | val_0_unsup_loss_numpy: 0.3901500105857849|  0:04:33s\n",
      "epoch 740| loss: 0.54278 | val_0_unsup_loss_numpy: 0.394679993391037|  0:04:35s\n",
      "epoch 745| loss: 0.53205 | val_0_unsup_loss_numpy: 0.39779001474380493|  0:04:37s\n",
      "epoch 750| loss: 0.5409  | val_0_unsup_loss_numpy: 0.39100998640060425|  0:04:39s\n",
      "epoch 755| loss: 0.53013 | val_0_unsup_loss_numpy: 0.3909800052642822|  0:04:41s\n",
      "epoch 760| loss: 0.5439  | val_0_unsup_loss_numpy: 0.3893899917602539|  0:04:43s\n",
      "epoch 765| loss: 0.52654 | val_0_unsup_loss_numpy: 0.38844001293182373|  0:04:44s\n",
      "epoch 770| loss: 0.53833 | val_0_unsup_loss_numpy: 0.3903299868106842|  0:04:46s\n",
      "epoch 775| loss: 0.53363 | val_0_unsup_loss_numpy: 0.3896999955177307|  0:04:48s\n",
      "epoch 780| loss: 0.54534 | val_0_unsup_loss_numpy: 0.3942199945449829|  0:04:50s\n",
      "epoch 785| loss: 0.55416 | val_0_unsup_loss_numpy: 0.3909499943256378|  0:04:52s\n",
      "epoch 790| loss: 0.53235 | val_0_unsup_loss_numpy: 0.38249000906944275|  0:04:54s\n",
      "epoch 795| loss: 0.53231 | val_0_unsup_loss_numpy: 0.3837999999523163|  0:04:56s\n",
      "epoch 800| loss: 0.52885 | val_0_unsup_loss_numpy: 0.3833000063896179|  0:04:58s\n",
      "epoch 805| loss: 0.53844 | val_0_unsup_loss_numpy: 0.38644999265670776|  0:05:00s\n",
      "epoch 810| loss: 0.5328  | val_0_unsup_loss_numpy: 0.3875400125980377|  0:05:02s\n",
      "epoch 815| loss: 0.55115 | val_0_unsup_loss_numpy: 0.3868800103664398|  0:05:03s\n",
      "epoch 820| loss: 0.55943 | val_0_unsup_loss_numpy: 0.3870300054550171|  0:05:05s\n",
      "epoch 825| loss: 0.53533 | val_0_unsup_loss_numpy: 0.38718000054359436|  0:05:07s\n",
      "epoch 830| loss: 0.54426 | val_0_unsup_loss_numpy: 0.388480007648468|  0:05:09s\n",
      "epoch 835| loss: 0.53751 | val_0_unsup_loss_numpy: 0.3832699954509735|  0:05:11s\n",
      "epoch 840| loss: 0.51381 | val_0_unsup_loss_numpy: 0.3846200108528137|  0:05:13s\n",
      "epoch 845| loss: 0.54787 | val_0_unsup_loss_numpy: 0.3822900056838989|  0:05:15s\n",
      "epoch 850| loss: 0.52882 | val_0_unsup_loss_numpy: 0.38558000326156616|  0:05:17s\n",
      "epoch 855| loss: 0.52922 | val_0_unsup_loss_numpy: 0.38312000036239624|  0:05:19s\n",
      "epoch 860| loss: 0.53903 | val_0_unsup_loss_numpy: 0.3831000030040741|  0:05:20s\n",
      "epoch 865| loss: 0.5301  | val_0_unsup_loss_numpy: 0.38060998916625977|  0:05:22s\n",
      "epoch 870| loss: 0.52748 | val_0_unsup_loss_numpy: 0.3848699927330017|  0:05:24s\n",
      "epoch 875| loss: 0.52731 | val_0_unsup_loss_numpy: 0.385560005903244|  0:05:26s\n",
      "epoch 880| loss: 0.53016 | val_0_unsup_loss_numpy: 0.3821299970149994|  0:05:28s\n",
      "epoch 885| loss: 0.52481 | val_0_unsup_loss_numpy: 0.3797000050544739|  0:05:30s\n",
      "epoch 890| loss: 0.5346  | val_0_unsup_loss_numpy: 0.38523000478744507|  0:05:32s\n",
      "epoch 895| loss: 0.5345  | val_0_unsup_loss_numpy: 0.37872999906539917|  0:05:34s\n",
      "epoch 900| loss: 0.53048 | val_0_unsup_loss_numpy: 0.3791100084781647|  0:05:35s\n",
      "epoch 905| loss: 0.51681 | val_0_unsup_loss_numpy: 0.38043999671936035|  0:05:37s\n",
      "epoch 910| loss: 0.53442 | val_0_unsup_loss_numpy: 0.3780499994754791|  0:05:39s\n",
      "epoch 915| loss: 0.5233  | val_0_unsup_loss_numpy: 0.38464000821113586|  0:05:41s\n",
      "epoch 920| loss: 0.53314 | val_0_unsup_loss_numpy: 0.3837200105190277|  0:05:43s\n",
      "epoch 925| loss: 0.51958 | val_0_unsup_loss_numpy: 0.38317999243736267|  0:05:45s\n",
      "epoch 930| loss: 0.52767 | val_0_unsup_loss_numpy: 0.38332998752593994|  0:05:47s\n",
      "epoch 935| loss: 0.53442 | val_0_unsup_loss_numpy: 0.379040002822876|  0:05:49s\n",
      "epoch 940| loss: 0.54281 | val_0_unsup_loss_numpy: 0.3822000026702881|  0:05:50s\n",
      "epoch 945| loss: 0.5052  | val_0_unsup_loss_numpy: 0.38082000613212585|  0:05:52s\n",
      "epoch 950| loss: 0.52397 | val_0_unsup_loss_numpy: 0.3793700039386749|  0:05:54s\n",
      "epoch 955| loss: 0.53487 | val_0_unsup_loss_numpy: 0.3837699890136719|  0:05:56s\n",
      "epoch 960| loss: 0.55447 | val_0_unsup_loss_numpy: 0.3815700113773346|  0:05:58s\n",
      "epoch 965| loss: 0.52011 | val_0_unsup_loss_numpy: 0.38418999314308167|  0:05:59s\n",
      "epoch 970| loss: 0.54235 | val_0_unsup_loss_numpy: 0.37724998593330383|  0:06:01s\n",
      "epoch 975| loss: 0.53234 | val_0_unsup_loss_numpy: 0.3802500069141388|  0:06:03s\n",
      "epoch 980| loss: 0.53578 | val_0_unsup_loss_numpy: 0.3909899890422821|  0:06:05s\n",
      "epoch 985| loss: 0.53057 | val_0_unsup_loss_numpy: 0.38951000571250916|  0:06:07s\n",
      "epoch 990| loss: 0.52563 | val_0_unsup_loss_numpy: 0.38541001081466675|  0:06:08s\n",
      "epoch 995| loss: 0.52152 | val_0_unsup_loss_numpy: 0.3856300115585327|  0:06:10s\n",
      "epoch 1000| loss: 0.54021 | val_0_unsup_loss_numpy: 0.3770599961280823|  0:06:12s\n",
      "epoch 1005| loss: 0.52702 | val_0_unsup_loss_numpy: 0.3862000107765198|  0:06:14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1010| loss: 0.55613 | val_0_unsup_loss_numpy: 0.39333000779151917|  0:06:16s\n",
      "epoch 1015| loss: 0.53448 | val_0_unsup_loss_numpy: 0.3737800121307373|  0:06:18s\n",
      "epoch 1020| loss: 0.52174 | val_0_unsup_loss_numpy: 0.3698900043964386|  0:06:20s\n",
      "epoch 1025| loss: 0.51175 | val_0_unsup_loss_numpy: 0.38106998801231384|  0:06:21s\n",
      "epoch 1030| loss: 0.53075 | val_0_unsup_loss_numpy: 0.3800300061702728|  0:06:23s\n",
      "epoch 1035| loss: 0.52976 | val_0_unsup_loss_numpy: 0.37564998865127563|  0:06:25s\n",
      "epoch 1040| loss: 0.5223  | val_0_unsup_loss_numpy: 0.3884899914264679|  0:06:27s\n",
      "epoch 1045| loss: 0.53956 | val_0_unsup_loss_numpy: 0.3829199969768524|  0:06:29s\n",
      "epoch 1050| loss: 0.54764 | val_0_unsup_loss_numpy: 0.38596999645233154|  0:06:31s\n",
      "epoch 1055| loss: 0.53472 | val_0_unsup_loss_numpy: 0.3803099989891052|  0:06:32s\n",
      "epoch 1060| loss: 0.53371 | val_0_unsup_loss_numpy: 0.37970998883247375|  0:06:34s\n",
      "epoch 1065| loss: 0.5107  | val_0_unsup_loss_numpy: 0.3782399892807007|  0:06:36s\n",
      "epoch 1070| loss: 0.51434 | val_0_unsup_loss_numpy: 0.38012999296188354|  0:06:38s\n",
      "epoch 1075| loss: 0.52661 | val_0_unsup_loss_numpy: 0.38194000720977783|  0:06:40s\n",
      "epoch 1080| loss: 0.51444 | val_0_unsup_loss_numpy: 0.3724299967288971|  0:06:42s\n",
      "epoch 1085| loss: 0.52463 | val_0_unsup_loss_numpy: 0.3720499873161316|  0:06:44s\n",
      "epoch 1090| loss: 0.52408 | val_0_unsup_loss_numpy: 0.3715899884700775|  0:06:46s\n",
      "epoch 1095| loss: 0.51016 | val_0_unsup_loss_numpy: 0.372979998588562|  0:06:48s\n",
      "epoch 1100| loss: 0.52653 | val_0_unsup_loss_numpy: 0.37292999029159546|  0:06:49s\n",
      "epoch 1105| loss: 0.51559 | val_0_unsup_loss_numpy: 0.37988999485969543|  0:06:51s\n",
      "epoch 1110| loss: 0.51345 | val_0_unsup_loss_numpy: 0.3903999924659729|  0:06:53s\n",
      "epoch 1115| loss: 0.52661 | val_0_unsup_loss_numpy: 0.37362000346183777|  0:06:55s\n",
      "epoch 1120| loss: 0.51746 | val_0_unsup_loss_numpy: 0.3743700087070465|  0:06:57s\n",
      "epoch 1125| loss: 0.53068 | val_0_unsup_loss_numpy: 0.37751999497413635|  0:06:59s\n",
      "epoch 1130| loss: 0.5265  | val_0_unsup_loss_numpy: 0.3776699900627136|  0:07:01s\n",
      "epoch 1135| loss: 0.55431 | val_0_unsup_loss_numpy: 0.37970998883247375|  0:07:03s\n",
      "epoch 1140| loss: 0.53738 | val_0_unsup_loss_numpy: 0.3864000141620636|  0:07:05s\n",
      "epoch 1145| loss: 0.53951 | val_0_unsup_loss_numpy: 0.37525999546051025|  0:07:07s\n",
      "epoch 1150| loss: 0.5263  | val_0_unsup_loss_numpy: 0.3756600022315979|  0:07:08s\n",
      "epoch 1155| loss: 0.52201 | val_0_unsup_loss_numpy: 0.36675000190734863|  0:07:10s\n",
      "epoch 1160| loss: 0.54041 | val_0_unsup_loss_numpy: 0.37547001242637634|  0:07:12s\n",
      "epoch 1165| loss: 0.53598 | val_0_unsup_loss_numpy: 0.37887001037597656|  0:07:14s\n",
      "epoch 1170| loss: 0.52493 | val_0_unsup_loss_numpy: 0.37720999121665955|  0:07:16s\n",
      "epoch 1175| loss: 0.51043 | val_0_unsup_loss_numpy: 0.37801000475883484|  0:07:18s\n",
      "epoch 1180| loss: 0.51284 | val_0_unsup_loss_numpy: 0.37529000639915466|  0:07:19s\n",
      "epoch 1185| loss: 0.527   | val_0_unsup_loss_numpy: 0.3683899939060211|  0:07:21s\n",
      "epoch 1190| loss: 0.50972 | val_0_unsup_loss_numpy: 0.37338000535964966|  0:07:23s\n",
      "epoch 1195| loss: 0.53684 | val_0_unsup_loss_numpy: 0.3693999946117401|  0:07:25s\n",
      "epoch 1200| loss: 0.52    | val_0_unsup_loss_numpy: 0.3720700144767761|  0:07:27s\n",
      "epoch 1205| loss: 0.53213 | val_0_unsup_loss_numpy: 0.37101998925209045|  0:07:29s\n",
      "epoch 1210| loss: 0.51219 | val_0_unsup_loss_numpy: 0.3705100119113922|  0:07:31s\n",
      "epoch 1215| loss: 0.52818 | val_0_unsup_loss_numpy: 0.37154000997543335|  0:07:32s\n",
      "epoch 1220| loss: 0.52561 | val_0_unsup_loss_numpy: 0.3725000023841858|  0:07:34s\n",
      "epoch 1225| loss: 0.5316  | val_0_unsup_loss_numpy: 0.3718299865722656|  0:07:36s\n",
      "epoch 1230| loss: 0.51532 | val_0_unsup_loss_numpy: 0.3721100091934204|  0:07:38s\n",
      "epoch 1235| loss: 0.52547 | val_0_unsup_loss_numpy: 0.36928001046180725|  0:07:39s\n",
      "epoch 1240| loss: 0.51295 | val_0_unsup_loss_numpy: 0.3698500096797943|  0:07:41s\n",
      "epoch 1245| loss: 0.52544 | val_0_unsup_loss_numpy: 0.3695800006389618|  0:07:43s\n",
      "epoch 1250| loss: 0.51931 | val_0_unsup_loss_numpy: 0.37224000692367554|  0:07:45s\n",
      "epoch 1255| loss: 0.51203 | val_0_unsup_loss_numpy: 0.3682999908924103|  0:07:47s\n",
      "epoch 1260| loss: 0.52392 | val_0_unsup_loss_numpy: 0.3722499907016754|  0:07:49s\n",
      "epoch 1265| loss: 0.52457 | val_0_unsup_loss_numpy: 0.37953999638557434|  0:07:50s\n",
      "epoch 1270| loss: 0.53573 | val_0_unsup_loss_numpy: 0.3753400146961212|  0:07:52s\n",
      "epoch 1275| loss: 0.51841 | val_0_unsup_loss_numpy: 0.3746899962425232|  0:07:54s\n",
      "epoch 1280| loss: 0.50908 | val_0_unsup_loss_numpy: 0.3728100061416626|  0:07:56s\n",
      "epoch 1285| loss: 0.51419 | val_0_unsup_loss_numpy: 0.3688400089740753|  0:07:57s\n",
      "epoch 1290| loss: 0.51712 | val_0_unsup_loss_numpy: 0.3678399920463562|  0:07:59s\n",
      "epoch 1295| loss: 0.49371 | val_0_unsup_loss_numpy: 0.37323999404907227|  0:08:01s\n",
      "epoch 1300| loss: 0.52785 | val_0_unsup_loss_numpy: 0.3711000084877014|  0:08:03s\n",
      "\n",
      "Early stopping occurred at epoch 1304 with best_epoch = 1154 and best_val_0_unsup_loss_numpy = 0.3666299879550934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosal/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "unsupervised_model.fit(\n",
    "    X_train=X_train.values,\n",
    "    eval_set=[X_test.values],\n",
    "    max_epochs=2000,\n",
    "    patience=150,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    pretraining_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "60ca072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./MODEL/PretrainedModel_0.3600.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./MODEL/PretrainedModel_0.3600.zip'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsupervised_model.save_model('./MODEL/PretrainedModel_0.3600')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735efec7",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "## Fine-tunning the Multi-label Classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5b65cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    cat_idxs=[idx for idx, nCat in enumerate(nCategory) if nCat <= 2],\n",
    "    cat_dims=[nCat for idx, nCat in enumerate(nCategory) if nCat <= 2],\n",
    "    cat_emb_dim=5,\n",
    "    optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2),\n",
    "    scheduler_fn=CosineAnnealingLR, scheduler_params={\"T_max\": 20, \"eta_min\": 1e-5}, \n",
    "    # scheduler_params={\"step_size\":50, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax' # \"sparsemax\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a7b0ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosal/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.8306  | train_logloss: 0.47772 | valid_logloss: 0.488   |  0:00:01s\n",
      "epoch 1  | loss: 0.4127  | train_logloss: 0.37586 | valid_logloss: 0.38641 |  0:00:03s\n",
      "epoch 2  | loss: 0.34546 | train_logloss: 0.32953 | valid_logloss: 0.33996 |  0:00:05s\n",
      "epoch 3  | loss: 0.32017 | train_logloss: 0.30274 | valid_logloss: 0.31295 |  0:00:06s\n",
      "epoch 4  | loss: 0.29874 | train_logloss: 0.28914 | valid_logloss: 0.29986 |  0:00:08s\n",
      "epoch 5  | loss: 0.28368 | train_logloss: 0.26872 | valid_logloss: 0.27983 |  0:00:10s\n",
      "epoch 6  | loss: 0.27262 | train_logloss: 0.26351 | valid_logloss: 0.27497 |  0:00:11s\n",
      "epoch 7  | loss: 0.26083 | train_logloss: 0.25024 | valid_logloss: 0.26228 |  0:00:13s\n",
      "epoch 8  | loss: 0.2502  | train_logloss: 0.23981 | valid_logloss: 0.25242 |  0:00:15s\n",
      "epoch 9  | loss: 0.24425 | train_logloss: 0.23222 | valid_logloss: 0.24382 |  0:00:16s\n",
      "epoch 10 | loss: 0.23964 | train_logloss: 0.22757 | valid_logloss: 0.23942 |  0:00:18s\n",
      "epoch 11 | loss: 0.23263 | train_logloss: 0.22432 | valid_logloss: 0.23767 |  0:00:20s\n",
      "epoch 12 | loss: 0.22907 | train_logloss: 0.22178 | valid_logloss: 0.2357  |  0:00:22s\n",
      "epoch 13 | loss: 0.22735 | train_logloss: 0.21827 | valid_logloss: 0.23111 |  0:00:23s\n",
      "epoch 14 | loss: 0.22611 | train_logloss: 0.21676 | valid_logloss: 0.22973 |  0:00:25s\n",
      "epoch 15 | loss: 0.22516 | train_logloss: 0.21698 | valid_logloss: 0.2304  |  0:00:27s\n",
      "epoch 16 | loss: 0.22267 | train_logloss: 0.21484 | valid_logloss: 0.22843 |  0:00:28s\n",
      "epoch 17 | loss: 0.22132 | train_logloss: 0.2144  | valid_logloss: 0.22791 |  0:00:30s\n",
      "epoch 18 | loss: 0.22202 | train_logloss: 0.21418 | valid_logloss: 0.22775 |  0:00:32s\n",
      "epoch 19 | loss: 0.22407 | train_logloss: 0.21522 | valid_logloss: 0.22885 |  0:00:33s\n",
      "epoch 20 | loss: 0.22207 | train_logloss: 0.21513 | valid_logloss: 0.22879 |  0:00:35s\n",
      "epoch 21 | loss: 0.22132 | train_logloss: 0.21422 | valid_logloss: 0.22771 |  0:00:37s\n",
      "epoch 22 | loss: 0.22019 | train_logloss: 0.21339 | valid_logloss: 0.22707 |  0:00:38s\n",
      "epoch 23 | loss: 0.21981 | train_logloss: 0.21291 | valid_logloss: 0.22668 |  0:00:40s\n",
      "epoch 24 | loss: 0.22073 | train_logloss: 0.21212 | valid_logloss: 0.22594 |  0:00:42s\n",
      "epoch 25 | loss: 0.22208 | train_logloss: 0.2118  | valid_logloss: 0.22552 |  0:00:44s\n",
      "epoch 26 | loss: 0.2205  | train_logloss: 0.2093  | valid_logloss: 0.22269 |  0:00:45s\n",
      "epoch 27 | loss: 0.21816 | train_logloss: 0.20908 | valid_logloss: 0.22257 |  0:00:47s\n",
      "epoch 28 | loss: 0.2177  | train_logloss: 0.20732 | valid_logloss: 0.2208  |  0:00:49s\n",
      "epoch 29 | loss: 0.21347 | train_logloss: 0.20464 | valid_logloss: 0.21781 |  0:00:50s\n",
      "epoch 30 | loss: 0.21202 | train_logloss: 0.20181 | valid_logloss: 0.21681 |  0:00:52s\n",
      "epoch 31 | loss: 0.21156 | train_logloss: 0.20132 | valid_logloss: 0.21423 |  0:00:54s\n",
      "epoch 32 | loss: 0.20899 | train_logloss: 0.19739 | valid_logloss: 0.21258 |  0:00:55s\n",
      "epoch 33 | loss: 0.20646 | train_logloss: 0.19482 | valid_logloss: 0.21    |  0:00:57s\n",
      "epoch 34 | loss: 0.20352 | train_logloss: 0.19225 | valid_logloss: 0.20623 |  0:00:59s\n",
      "epoch 35 | loss: 0.20494 | train_logloss: 0.19394 | valid_logloss: 0.21099 |  0:01:01s\n",
      "epoch 36 | loss: 0.20584 | train_logloss: 0.19541 | valid_logloss: 0.21083 |  0:01:02s\n",
      "epoch 37 | loss: 0.20098 | train_logloss: 0.18935 | valid_logloss: 0.2041  |  0:01:04s\n",
      "epoch 38 | loss: 0.19984 | train_logloss: 0.19004 | valid_logloss: 0.20476 |  0:01:06s\n",
      "epoch 39 | loss: 0.19833 | train_logloss: 0.18706 | valid_logloss: 0.20247 |  0:01:07s\n",
      "epoch 40 | loss: 0.19592 | train_logloss: 0.18572 | valid_logloss: 0.19845 |  0:01:09s\n",
      "epoch 41 | loss: 0.19281 | train_logloss: 0.18103 | valid_logloss: 0.19388 |  0:01:11s\n",
      "epoch 42 | loss: 0.19344 | train_logloss: 0.18313 | valid_logloss: 0.19686 |  0:01:12s\n",
      "epoch 43 | loss: 0.19987 | train_logloss: 0.18109 | valid_logloss: 0.19714 |  0:01:14s\n",
      "epoch 44 | loss: 0.18885 | train_logloss: 0.17778 | valid_logloss: 0.19232 |  0:01:16s\n",
      "epoch 45 | loss: 0.19056 | train_logloss: 0.179   | valid_logloss: 0.19182 |  0:01:17s\n",
      "epoch 46 | loss: 0.18845 | train_logloss: 0.17682 | valid_logloss: 0.19071 |  0:01:19s\n",
      "epoch 47 | loss: 0.18537 | train_logloss: 0.17489 | valid_logloss: 0.18865 |  0:01:21s\n",
      "epoch 48 | loss: 0.18669 | train_logloss: 0.1749  | valid_logloss: 0.1896  |  0:01:22s\n",
      "epoch 49 | loss: 0.18353 | train_logloss: 0.17375 | valid_logloss: 0.18798 |  0:01:24s\n",
      "epoch 50 | loss: 0.18159 | train_logloss: 0.17329 | valid_logloss: 0.18843 |  0:01:26s\n",
      "epoch 51 | loss: 0.18034 | train_logloss: 0.17066 | valid_logloss: 0.18392 |  0:01:28s\n",
      "epoch 52 | loss: 0.17882 | train_logloss: 0.17086 | valid_logloss: 0.18493 |  0:01:29s\n",
      "epoch 53 | loss: 0.18076 | train_logloss: 0.17035 | valid_logloss: 0.18466 |  0:01:31s\n",
      "epoch 54 | loss: 0.17881 | train_logloss: 0.1691  | valid_logloss: 0.18264 |  0:01:33s\n",
      "epoch 55 | loss: 0.1792  | train_logloss: 0.16929 | valid_logloss: 0.1826  |  0:01:34s\n",
      "epoch 56 | loss: 0.17819 | train_logloss: 0.16885 | valid_logloss: 0.18322 |  0:01:36s\n",
      "epoch 57 | loss: 0.17835 | train_logloss: 0.16848 | valid_logloss: 0.18237 |  0:01:38s\n",
      "epoch 58 | loss: 0.17677 | train_logloss: 0.16847 | valid_logloss: 0.18237 |  0:01:39s\n",
      "epoch 59 | loss: 0.17792 | train_logloss: 0.16841 | valid_logloss: 0.1838  |  0:01:41s\n",
      "epoch 60 | loss: 0.17734 | train_logloss: 0.1685  | valid_logloss: 0.1836  |  0:01:43s\n",
      "epoch 61 | loss: 0.17677 | train_logloss: 0.16838 | valid_logloss: 0.18239 |  0:01:44s\n",
      "epoch 62 | loss: 0.1782  | train_logloss: 0.16839 | valid_logloss: 0.18234 |  0:01:46s\n",
      "epoch 63 | loss: 0.17739 | train_logloss: 0.16814 | valid_logloss: 0.18195 |  0:01:48s\n",
      "epoch 64 | loss: 0.17758 | train_logloss: 0.16795 | valid_logloss: 0.18223 |  0:01:50s\n",
      "epoch 65 | loss: 0.1766  | train_logloss: 0.1678  | valid_logloss: 0.18284 |  0:01:51s\n",
      "epoch 66 | loss: 0.17806 | train_logloss: 0.16907 | valid_logloss: 0.18434 |  0:01:53s\n",
      "epoch 67 | loss: 0.18043 | train_logloss: 0.16955 | valid_logloss: 0.18512 |  0:01:55s\n",
      "epoch 68 | loss: 0.17696 | train_logloss: 0.16844 | valid_logloss: 0.18347 |  0:01:56s\n",
      "epoch 69 | loss: 0.17806 | train_logloss: 0.16813 | valid_logloss: 0.18388 |  0:01:58s\n",
      "epoch 70 | loss: 0.1818  | train_logloss: 0.16864 | valid_logloss: 0.18369 |  0:02:00s\n",
      "epoch 71 | loss: 0.18144 | train_logloss: 0.1689  | valid_logloss: 0.18475 |  0:02:01s\n",
      "epoch 72 | loss: 0.18048 | train_logloss: 0.16929 | valid_logloss: 0.18384 |  0:02:03s\n",
      "epoch 73 | loss: 0.18025 | train_logloss: 0.17068 | valid_logloss: 0.18699 |  0:02:05s\n",
      "epoch 74 | loss: 0.18076 | train_logloss: 0.16942 | valid_logloss: 0.18504 |  0:02:07s\n",
      "epoch 75 | loss: 0.1811  | train_logloss: 0.16798 | valid_logloss: 0.18287 |  0:02:08s\n",
      "epoch 76 | loss: 0.17769 | train_logloss: 0.16976 | valid_logloss: 0.18468 |  0:02:10s\n",
      "epoch 77 | loss: 0.17876 | train_logloss: 0.16931 | valid_logloss: 0.18499 |  0:02:12s\n",
      "epoch 78 | loss: 0.18007 | train_logloss: 0.17022 | valid_logloss: 0.18637 |  0:02:13s\n",
      "epoch 79 | loss: 0.17712 | train_logloss: 0.16726 | valid_logloss: 0.1832  |  0:02:15s\n",
      "epoch 80 | loss: 0.18007 | train_logloss: 0.1683  | valid_logloss: 0.18398 |  0:02:17s\n",
      "epoch 81 | loss: 0.18081 | train_logloss: 0.16829 | valid_logloss: 0.18427 |  0:02:18s\n",
      "epoch 82 | loss: 0.17889 | train_logloss: 0.16594 | valid_logloss: 0.18059 |  0:02:20s\n",
      "epoch 83 | loss: 0.17553 | train_logloss: 0.16779 | valid_logloss: 0.1827  |  0:02:22s\n",
      "epoch 84 | loss: 0.17523 | train_logloss: 0.16594 | valid_logloss: 0.18205 |  0:02:24s\n",
      "epoch 85 | loss: 0.17561 | train_logloss: 0.16359 | valid_logloss: 0.17837 |  0:02:25s\n",
      "epoch 86 | loss: 0.17454 | train_logloss: 0.16508 | valid_logloss: 0.17922 |  0:02:27s\n",
      "epoch 87 | loss: 0.1752  | train_logloss: 0.16474 | valid_logloss: 0.17977 |  0:02:29s\n",
      "epoch 88 | loss: 0.17645 | train_logloss: 0.16435 | valid_logloss: 0.17864 |  0:02:30s\n",
      "epoch 89 | loss: 0.17317 | train_logloss: 0.16409 | valid_logloss: 0.17948 |  0:02:32s\n",
      "epoch 90 | loss: 0.17455 | train_logloss: 0.16493 | valid_logloss: 0.17985 |  0:02:34s\n",
      "epoch 91 | loss: 0.1766  | train_logloss: 0.16279 | valid_logloss: 0.17823 |  0:02:35s\n",
      "epoch 92 | loss: 0.1725  | train_logloss: 0.16242 | valid_logloss: 0.1776  |  0:02:37s\n",
      "epoch 93 | loss: 0.17069 | train_logloss: 0.16151 | valid_logloss: 0.17693 |  0:02:39s\n",
      "epoch 94 | loss: 0.17099 | train_logloss: 0.16094 | valid_logloss: 0.17603 |  0:02:40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95 | loss: 0.17128 | train_logloss: 0.1608  | valid_logloss: 0.17588 |  0:02:42s\n",
      "epoch 96 | loss: 0.17058 | train_logloss: 0.16094 | valid_logloss: 0.17637 |  0:02:44s\n",
      "epoch 97 | loss: 0.16989 | train_logloss: 0.16094 | valid_logloss: 0.17618 |  0:02:45s\n",
      "epoch 98 | loss: 0.17077 | train_logloss: 0.16116 | valid_logloss: 0.17635 |  0:02:47s\n",
      "epoch 99 | loss: 0.16978 | train_logloss: 0.16119 | valid_logloss: 0.17657 |  0:02:49s\n",
      "epoch 100| loss: 0.16809 | train_logloss: 0.16086 | valid_logloss: 0.17627 |  0:02:50s\n",
      "epoch 101| loss: 0.16996 | train_logloss: 0.16082 | valid_logloss: 0.1762  |  0:02:52s\n",
      "epoch 102| loss: 0.16902 | train_logloss: 0.1606  | valid_logloss: 0.17595 |  0:02:54s\n",
      "epoch 103| loss: 0.1698  | train_logloss: 0.16045 | valid_logloss: 0.17594 |  0:02:55s\n",
      "epoch 104| loss: 0.16992 | train_logloss: 0.16048 | valid_logloss: 0.17566 |  0:02:57s\n",
      "epoch 105| loss: 0.16953 | train_logloss: 0.16023 | valid_logloss: 0.17536 |  0:02:59s\n",
      "epoch 106| loss: 0.17104 | train_logloss: 0.16047 | valid_logloss: 0.17509 |  0:03:01s\n",
      "epoch 107| loss: 0.17073 | train_logloss: 0.16034 | valid_logloss: 0.17555 |  0:03:02s\n",
      "epoch 108| loss: 0.16961 | train_logloss: 0.16092 | valid_logloss: 0.17579 |  0:03:04s\n",
      "epoch 109| loss: 0.17084 | train_logloss: 0.16266 | valid_logloss: 0.17653 |  0:03:06s\n",
      "epoch 110| loss: 0.17198 | train_logloss: 0.16172 | valid_logloss: 0.17638 |  0:03:07s\n",
      "epoch 111| loss: 0.17457 | train_logloss: 0.16145 | valid_logloss: 0.17698 |  0:03:09s\n",
      "epoch 112| loss: 0.17448 | train_logloss: 0.16251 | valid_logloss: 0.17677 |  0:03:11s\n",
      "epoch 113| loss: 0.17231 | train_logloss: 0.16143 | valid_logloss: 0.17607 |  0:03:12s\n",
      "epoch 114| loss: 0.17155 | train_logloss: 0.1617  | valid_logloss: 0.17617 |  0:03:14s\n",
      "epoch 115| loss: 0.17621 | train_logloss: 0.16259 | valid_logloss: 0.17717 |  0:03:16s\n",
      "epoch 116| loss: 0.1745  | train_logloss: 0.16175 | valid_logloss: 0.17583 |  0:03:17s\n",
      "epoch 117| loss: 0.17154 | train_logloss: 0.16232 | valid_logloss: 0.17567 |  0:03:19s\n",
      "epoch 118| loss: 0.17552 | train_logloss: 0.16171 | valid_logloss: 0.17326 |  0:03:21s\n",
      "epoch 119| loss: 0.1735  | train_logloss: 0.16129 | valid_logloss: 0.1734  |  0:03:23s\n",
      "epoch 120| loss: 0.17338 | train_logloss: 0.16181 | valid_logloss: 0.17358 |  0:03:24s\n",
      "epoch 121| loss: 0.17265 | train_logloss: 0.16054 | valid_logloss: 0.17168 |  0:03:26s\n",
      "epoch 122| loss: 0.17078 | train_logloss: 0.16051 | valid_logloss: 0.17298 |  0:03:28s\n",
      "epoch 123| loss: 0.17666 | train_logloss: 0.16158 | valid_logloss: 0.1739  |  0:03:29s\n",
      "epoch 124| loss: 0.17217 | train_logloss: 0.16121 | valid_logloss: 0.17398 |  0:03:31s\n",
      "epoch 125| loss: 0.17195 | train_logloss: 0.16193 | valid_logloss: 0.17404 |  0:03:33s\n",
      "epoch 126| loss: 0.17364 | train_logloss: 0.16078 | valid_logloss: 0.1725  |  0:03:34s\n",
      "epoch 127| loss: 0.17101 | train_logloss: 0.15983 | valid_logloss: 0.17214 |  0:03:36s\n",
      "epoch 128| loss: 0.17094 | train_logloss: 0.15937 | valid_logloss: 0.17164 |  0:03:38s\n",
      "epoch 129| loss: 0.168   | train_logloss: 0.1584  | valid_logloss: 0.16972 |  0:03:40s\n",
      "epoch 130| loss: 0.17009 | train_logloss: 0.15799 | valid_logloss: 0.16827 |  0:03:41s\n",
      "epoch 131| loss: 0.16704 | train_logloss: 0.15688 | valid_logloss: 0.16744 |  0:03:43s\n",
      "epoch 132| loss: 0.16869 | train_logloss: 0.15648 | valid_logloss: 0.16728 |  0:03:45s\n",
      "epoch 133| loss: 0.16554 | train_logloss: 0.15648 | valid_logloss: 0.16804 |  0:03:46s\n",
      "epoch 134| loss: 0.16655 | train_logloss: 0.15556 | valid_logloss: 0.16639 |  0:03:48s\n",
      "epoch 135| loss: 0.1671  | train_logloss: 0.15581 | valid_logloss: 0.16711 |  0:03:50s\n",
      "epoch 136| loss: 0.16437 | train_logloss: 0.1558  | valid_logloss: 0.16705 |  0:03:51s\n",
      "epoch 137| loss: 0.16487 | train_logloss: 0.15582 | valid_logloss: 0.16708 |  0:03:53s\n",
      "epoch 138| loss: 0.16459 | train_logloss: 0.1556  | valid_logloss: 0.16678 |  0:03:55s\n",
      "epoch 139| loss: 0.16525 | train_logloss: 0.15604 | valid_logloss: 0.16691 |  0:03:56s\n",
      "epoch 140| loss: 0.16496 | train_logloss: 0.15575 | valid_logloss: 0.16694 |  0:03:58s\n",
      "epoch 141| loss: 0.16525 | train_logloss: 0.15559 | valid_logloss: 0.16671 |  0:04:00s\n",
      "epoch 142| loss: 0.16402 | train_logloss: 0.1557  | valid_logloss: 0.16661 |  0:04:02s\n",
      "epoch 143| loss: 0.16394 | train_logloss: 0.1552  | valid_logloss: 0.16641 |  0:04:03s\n",
      "epoch 144| loss: 0.16434 | train_logloss: 0.15526 | valid_logloss: 0.16708 |  0:04:05s\n",
      "epoch 145| loss: 0.1649  | train_logloss: 0.15522 | valid_logloss: 0.1661  |  0:04:07s\n",
      "epoch 146| loss: 0.16583 | train_logloss: 0.15523 | valid_logloss: 0.16602 |  0:04:08s\n",
      "epoch 147| loss: 0.16621 | train_logloss: 0.1559  | valid_logloss: 0.16714 |  0:04:10s\n",
      "epoch 148| loss: 0.16611 | train_logloss: 0.15547 | valid_logloss: 0.1682  |  0:04:12s\n",
      "epoch 149| loss: 0.16867 | train_logloss: 0.1556  | valid_logloss: 0.16707 |  0:04:13s\n",
      "epoch 150| loss: 0.16625 | train_logloss: 0.1559  | valid_logloss: 0.1693  |  0:04:15s\n",
      "epoch 151| loss: 0.16547 | train_logloss: 0.15527 | valid_logloss: 0.16891 |  0:04:17s\n",
      "epoch 152| loss: 0.1676  | train_logloss: 0.15613 | valid_logloss: 0.17005 |  0:04:18s\n",
      "epoch 153| loss: 0.16545 | train_logloss: 0.15738 | valid_logloss: 0.17022 |  0:04:20s\n",
      "epoch 154| loss: 0.16709 | train_logloss: 0.15678 | valid_logloss: 0.1703  |  0:04:22s\n",
      "epoch 155| loss: 0.1689  | train_logloss: 0.15563 | valid_logloss: 0.1688  |  0:04:24s\n",
      "epoch 156| loss: 0.16569 | train_logloss: 0.15518 | valid_logloss: 0.16804 |  0:04:25s\n",
      "epoch 157| loss: 0.16631 | train_logloss: 0.15756 | valid_logloss: 0.17009 |  0:04:27s\n",
      "epoch 158| loss: 0.16593 | train_logloss: 0.15663 | valid_logloss: 0.16788 |  0:04:29s\n",
      "epoch 159| loss: 0.1687  | train_logloss: 0.15632 | valid_logloss: 0.16876 |  0:04:30s\n",
      "epoch 160| loss: 0.16562 | train_logloss: 0.15653 | valid_logloss: 0.16911 |  0:04:32s\n",
      "epoch 161| loss: 0.16644 | train_logloss: 0.15713 | valid_logloss: 0.16937 |  0:04:34s\n",
      "epoch 162| loss: 0.16773 | train_logloss: 0.15832 | valid_logloss: 0.17163 |  0:04:35s\n",
      "epoch 163| loss: 0.16718 | train_logloss: 0.15668 | valid_logloss: 0.16816 |  0:04:37s\n",
      "epoch 164| loss: 0.16679 | train_logloss: 0.15497 | valid_logloss: 0.16684 |  0:04:39s\n",
      "epoch 165| loss: 0.16654 | train_logloss: 0.15836 | valid_logloss: 0.17046 |  0:04:41s\n",
      "epoch 166| loss: 0.16689 | train_logloss: 0.1559  | valid_logloss: 0.16752 |  0:04:42s\n",
      "epoch 167| loss: 0.16674 | train_logloss: 0.15537 | valid_logloss: 0.16767 |  0:04:44s\n",
      "epoch 168| loss: 0.16562 | train_logloss: 0.15474 | valid_logloss: 0.16694 |  0:04:46s\n",
      "epoch 169| loss: 0.16728 | train_logloss: 0.15445 | valid_logloss: 0.16665 |  0:04:47s\n",
      "epoch 170| loss: 0.16548 | train_logloss: 0.1534  | valid_logloss: 0.16553 |  0:04:49s\n",
      "epoch 171| loss: 0.16586 | train_logloss: 0.1538  | valid_logloss: 0.1665  |  0:04:51s\n",
      "epoch 172| loss: 0.16417 | train_logloss: 0.15313 | valid_logloss: 0.16506 |  0:04:52s\n",
      "epoch 173| loss: 0.16336 | train_logloss: 0.15317 | valid_logloss: 0.16583 |  0:04:54s\n",
      "epoch 174| loss: 0.16113 | train_logloss: 0.15286 | valid_logloss: 0.16545 |  0:04:56s\n",
      "epoch 175| loss: 0.16104 | train_logloss: 0.15302 | valid_logloss: 0.16575 |  0:04:57s\n",
      "epoch 176| loss: 0.1617  | train_logloss: 0.15253 | valid_logloss: 0.16466 |  0:04:59s\n",
      "epoch 177| loss: 0.16227 | train_logloss: 0.15213 | valid_logloss: 0.1642  |  0:05:01s\n",
      "epoch 178| loss: 0.16178 | train_logloss: 0.15245 | valid_logloss: 0.16461 |  0:05:02s\n",
      "epoch 179| loss: 0.16187 | train_logloss: 0.15235 | valid_logloss: 0.16458 |  0:05:04s\n",
      "epoch 180| loss: 0.16197 | train_logloss: 0.15228 | valid_logloss: 0.16463 |  0:05:06s\n",
      "epoch 181| loss: 0.16077 | train_logloss: 0.15181 | valid_logloss: 0.1643  |  0:05:08s\n",
      "epoch 182| loss: 0.16105 | train_logloss: 0.15223 | valid_logloss: 0.16437 |  0:05:09s\n",
      "epoch 183| loss: 0.15974 | train_logloss: 0.1518  | valid_logloss: 0.16394 |  0:05:11s\n",
      "epoch 184| loss: 0.16127 | train_logloss: 0.15222 | valid_logloss: 0.16419 |  0:05:13s\n",
      "epoch 185| loss: 0.1596  | train_logloss: 0.15159 | valid_logloss: 0.16362 |  0:05:14s\n",
      "epoch 186| loss: 0.16242 | train_logloss: 0.15152 | valid_logloss: 0.16315 |  0:05:16s\n",
      "epoch 187| loss: 0.16354 | train_logloss: 0.15194 | valid_logloss: 0.16355 |  0:05:18s\n",
      "epoch 188| loss: 0.16157 | train_logloss: 0.1526  | valid_logloss: 0.16543 |  0:05:19s\n",
      "epoch 189| loss: 0.16263 | train_logloss: 0.15227 | valid_logloss: 0.16444 |  0:05:21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 190| loss: 0.16348 | train_logloss: 0.15357 | valid_logloss: 0.16482 |  0:05:23s\n",
      "epoch 191| loss: 0.16221 | train_logloss: 0.15253 | valid_logloss: 0.16412 |  0:05:24s\n",
      "epoch 192| loss: 0.16436 | train_logloss: 0.15328 | valid_logloss: 0.16495 |  0:05:26s\n",
      "epoch 193| loss: 0.16345 | train_logloss: 0.15379 | valid_logloss: 0.16532 |  0:05:28s\n",
      "epoch 194| loss: 0.16873 | train_logloss: 0.1543  | valid_logloss: 0.16516 |  0:05:29s\n",
      "epoch 195| loss: 0.16877 | train_logloss: 0.16064 | valid_logloss: 0.17094 |  0:05:31s\n",
      "epoch 196| loss: 0.16916 | train_logloss: 0.15976 | valid_logloss: 0.16977 |  0:05:33s\n",
      "epoch 197| loss: 0.17013 | train_logloss: 0.15569 | valid_logloss: 0.16634 |  0:05:35s\n",
      "epoch 198| loss: 0.16662 | train_logloss: 0.15514 | valid_logloss: 0.16671 |  0:05:36s\n",
      "epoch 199| loss: 0.16494 | train_logloss: 0.15483 | valid_logloss: 0.16956 |  0:05:38s\n",
      "epoch 200| loss: 0.17041 | train_logloss: 0.15657 | valid_logloss: 0.16895 |  0:05:40s\n",
      "epoch 201| loss: 0.1671  | train_logloss: 0.15695 | valid_logloss: 0.16798 |  0:05:41s\n",
      "epoch 202| loss: 0.16778 | train_logloss: 0.15575 | valid_logloss: 0.16763 |  0:05:43s\n",
      "epoch 203| loss: 0.16767 | train_logloss: 0.1561  | valid_logloss: 0.16788 |  0:05:45s\n",
      "epoch 204| loss: 0.16556 | train_logloss: 0.15452 | valid_logloss: 0.16719 |  0:05:46s\n",
      "epoch 205| loss: 0.1671  | train_logloss: 0.15526 | valid_logloss: 0.1669  |  0:05:48s\n",
      "epoch 206| loss: 0.1691  | train_logloss: 0.1572  | valid_logloss: 0.16855 |  0:05:50s\n",
      "epoch 207| loss: 0.16466 | train_logloss: 0.15403 | valid_logloss: 0.16529 |  0:05:51s\n",
      "epoch 208| loss: 0.16396 | train_logloss: 0.15624 | valid_logloss: 0.16836 |  0:05:53s\n",
      "epoch 209| loss: 0.16578 | train_logloss: 0.15374 | valid_logloss: 0.1664  |  0:05:55s\n",
      "epoch 210| loss: 0.16345 | train_logloss: 0.15406 | valid_logloss: 0.16592 |  0:05:56s\n",
      "epoch 211| loss: 0.16268 | train_logloss: 0.15302 | valid_logloss: 0.16564 |  0:05:58s\n",
      "epoch 212| loss: 0.16223 | train_logloss: 0.15219 | valid_logloss: 0.16416 |  0:06:00s\n",
      "epoch 213| loss: 0.16079 | train_logloss: 0.15154 | valid_logloss: 0.16331 |  0:06:01s\n",
      "epoch 214| loss: 0.15967 | train_logloss: 0.1517  | valid_logloss: 0.16341 |  0:06:03s\n",
      "epoch 215| loss: 0.1597  | train_logloss: 0.15123 | valid_logloss: 0.16294 |  0:06:05s\n",
      "epoch 216| loss: 0.15959 | train_logloss: 0.15102 | valid_logloss: 0.16283 |  0:06:07s\n",
      "epoch 217| loss: 0.15899 | train_logloss: 0.15115 | valid_logloss: 0.16297 |  0:06:08s\n",
      "epoch 218| loss: 0.15865 | train_logloss: 0.15094 | valid_logloss: 0.16279 |  0:06:10s\n",
      "epoch 219| loss: 0.15867 | train_logloss: 0.15096 | valid_logloss: 0.16306 |  0:06:12s\n",
      "epoch 220| loss: 0.15952 | train_logloss: 0.15117 | valid_logloss: 0.16308 |  0:06:13s\n",
      "epoch 221| loss: 0.15849 | train_logloss: 0.1509  | valid_logloss: 0.16288 |  0:06:15s\n",
      "epoch 222| loss: 0.16054 | train_logloss: 0.15092 | valid_logloss: 0.16296 |  0:06:17s\n",
      "epoch 223| loss: 0.15924 | train_logloss: 0.15073 | valid_logloss: 0.16281 |  0:06:18s\n",
      "epoch 224| loss: 0.15823 | train_logloss: 0.15085 | valid_logloss: 0.16307 |  0:06:20s\n",
      "epoch 225| loss: 0.15944 | train_logloss: 0.15088 | valid_logloss: 0.16273 |  0:06:22s\n",
      "epoch 226| loss: 0.16233 | train_logloss: 0.15191 | valid_logloss: 0.16383 |  0:06:23s\n",
      "epoch 227| loss: 0.15922 | train_logloss: 0.1515  | valid_logloss: 0.16383 |  0:06:25s\n",
      "epoch 228| loss: 0.15999 | train_logloss: 0.15107 | valid_logloss: 0.16375 |  0:06:27s\n",
      "epoch 229| loss: 0.16108 | train_logloss: 0.15077 | valid_logloss: 0.16183 |  0:06:29s\n",
      "epoch 230| loss: 0.16068 | train_logloss: 0.15217 | valid_logloss: 0.16377 |  0:06:30s\n",
      "epoch 231| loss: 0.16328 | train_logloss: 0.15223 | valid_logloss: 0.16402 |  0:06:32s\n",
      "epoch 232| loss: 0.16032 | train_logloss: 0.15061 | valid_logloss: 0.16308 |  0:06:34s\n",
      "epoch 233| loss: 0.16101 | train_logloss: 0.15142 | valid_logloss: 0.1641  |  0:06:35s\n",
      "epoch 234| loss: 0.16104 | train_logloss: 0.1536  | valid_logloss: 0.16568 |  0:06:37s\n",
      "epoch 235| loss: 0.1624  | train_logloss: 0.15189 | valid_logloss: 0.16623 |  0:06:39s\n",
      "epoch 236| loss: 0.16384 | train_logloss: 0.15419 | valid_logloss: 0.16789 |  0:06:40s\n",
      "epoch 237| loss: 0.16282 | train_logloss: 0.15426 | valid_logloss: 0.16771 |  0:06:42s\n",
      "epoch 238| loss: 0.16238 | train_logloss: 0.15617 | valid_logloss: 0.17031 |  0:06:44s\n",
      "epoch 239| loss: 0.16305 | train_logloss: 0.15187 | valid_logloss: 0.16631 |  0:06:45s\n",
      "epoch 240| loss: 0.16204 | train_logloss: 0.1521  | valid_logloss: 0.16482 |  0:06:47s\n",
      "epoch 241| loss: 0.16588 | train_logloss: 0.15275 | valid_logloss: 0.16516 |  0:06:49s\n",
      "epoch 242| loss: 0.16267 | train_logloss: 0.15444 | valid_logloss: 0.16714 |  0:06:51s\n",
      "epoch 243| loss: 0.16626 | train_logloss: 0.15319 | valid_logloss: 0.16799 |  0:06:52s\n",
      "epoch 244| loss: 0.16388 | train_logloss: 0.15205 | valid_logloss: 0.1651  |  0:06:54s\n",
      "epoch 245| loss: 0.16311 | train_logloss: 0.15198 | valid_logloss: 0.16662 |  0:06:56s\n",
      "epoch 246| loss: 0.16415 | train_logloss: 0.15235 | valid_logloss: 0.16478 |  0:06:57s\n",
      "epoch 247| loss: 0.16232 | train_logloss: 0.1517  | valid_logloss: 0.16371 |  0:06:59s\n",
      "epoch 248| loss: 0.1625  | train_logloss: 0.15257 | valid_logloss: 0.16484 |  0:07:01s\n",
      "epoch 249| loss: 0.16365 | train_logloss: 0.15373 | valid_logloss: 0.16724 |  0:07:02s\n",
      "epoch 250| loss: 0.16231 | train_logloss: 0.15187 | valid_logloss: 0.16454 |  0:07:04s\n",
      "epoch 251| loss: 0.15965 | train_logloss: 0.15123 | valid_logloss: 0.16286 |  0:07:06s\n",
      "epoch 252| loss: 0.16133 | train_logloss: 0.1508  | valid_logloss: 0.16216 |  0:07:07s\n",
      "epoch 253| loss: 0.16074 | train_logloss: 0.15041 | valid_logloss: 0.16133 |  0:07:09s\n",
      "epoch 254| loss: 0.15914 | train_logloss: 0.15009 | valid_logloss: 0.16156 |  0:07:11s\n",
      "epoch 255| loss: 0.15772 | train_logloss: 0.14944 | valid_logloss: 0.16133 |  0:07:13s\n",
      "epoch 256| loss: 0.15802 | train_logloss: 0.14953 | valid_logloss: 0.16111 |  0:07:14s\n",
      "epoch 257| loss: 0.16096 | train_logloss: 0.14947 | valid_logloss: 0.16168 |  0:07:16s\n",
      "epoch 258| loss: 0.15877 | train_logloss: 0.1497  | valid_logloss: 0.16182 |  0:07:18s\n",
      "epoch 259| loss: 0.1577  | train_logloss: 0.14941 | valid_logloss: 0.16144 |  0:07:19s\n",
      "epoch 260| loss: 0.1601  | train_logloss: 0.14972 | valid_logloss: 0.16195 |  0:07:21s\n",
      "epoch 261| loss: 0.15764 | train_logloss: 0.14967 | valid_logloss: 0.16178 |  0:07:23s\n",
      "epoch 262| loss: 0.15835 | train_logloss: 0.15001 | valid_logloss: 0.16201 |  0:07:24s\n",
      "epoch 263| loss: 0.15645 | train_logloss: 0.14962 | valid_logloss: 0.16186 |  0:07:26s\n",
      "epoch 264| loss: 0.15699 | train_logloss: 0.14898 | valid_logloss: 0.16113 |  0:07:28s\n",
      "epoch 265| loss: 0.15867 | train_logloss: 0.14921 | valid_logloss: 0.16125 |  0:07:29s\n",
      "epoch 266| loss: 0.15689 | train_logloss: 0.14902 | valid_logloss: 0.16116 |  0:07:31s\n",
      "epoch 267| loss: 0.15726 | train_logloss: 0.14958 | valid_logloss: 0.16135 |  0:07:33s\n",
      "epoch 268| loss: 0.15936 | train_logloss: 0.14889 | valid_logloss: 0.16108 |  0:07:34s\n",
      "epoch 269| loss: 0.15885 | train_logloss: 0.14974 | valid_logloss: 0.16258 |  0:07:36s\n",
      "epoch 270| loss: 0.16074 | train_logloss: 0.14999 | valid_logloss: 0.1628  |  0:07:38s\n",
      "epoch 271| loss: 0.15944 | train_logloss: 0.1495  | valid_logloss: 0.1622  |  0:07:40s\n",
      "epoch 272| loss: 0.15955 | train_logloss: 0.14905 | valid_logloss: 0.16159 |  0:07:41s\n",
      "epoch 273| loss: 0.1609  | train_logloss: 0.15101 | valid_logloss: 0.16289 |  0:07:43s\n",
      "epoch 274| loss: 0.16258 | train_logloss: 0.15086 | valid_logloss: 0.16411 |  0:07:45s\n",
      "epoch 275| loss: 0.16251 | train_logloss: 0.15401 | valid_logloss: 0.16614 |  0:07:46s\n",
      "epoch 276| loss: 0.16407 | train_logloss: 0.15421 | valid_logloss: 0.16499 |  0:07:48s\n",
      "epoch 277| loss: 0.16535 | train_logloss: 0.15203 | valid_logloss: 0.16369 |  0:07:50s\n",
      "epoch 278| loss: 0.16102 | train_logloss: 0.1513  | valid_logloss: 0.16354 |  0:07:51s\n",
      "epoch 279| loss: 0.16016 | train_logloss: 0.15071 | valid_logloss: 0.16143 |  0:07:53s\n",
      "epoch 280| loss: 0.16151 | train_logloss: 0.15155 | valid_logloss: 0.16346 |  0:07:55s\n",
      "epoch 281| loss: 0.164   | train_logloss: 0.15127 | valid_logloss: 0.16309 |  0:07:56s\n",
      "epoch 282| loss: 0.16239 | train_logloss: 0.15085 | valid_logloss: 0.1619  |  0:07:58s\n",
      "epoch 283| loss: 0.16362 | train_logloss: 0.15108 | valid_logloss: 0.16394 |  0:08:00s\n",
      "epoch 284| loss: 0.1652  | train_logloss: 0.15385 | valid_logloss: 0.16687 |  0:08:01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 285| loss: 0.16395 | train_logloss: 0.15204 | valid_logloss: 0.16483 |  0:08:03s\n",
      "epoch 286| loss: 0.16149 | train_logloss: 0.15301 | valid_logloss: 0.16336 |  0:08:05s\n",
      "epoch 287| loss: 0.16313 | train_logloss: 0.15205 | valid_logloss: 0.16348 |  0:08:07s\n",
      "epoch 288| loss: 0.16131 | train_logloss: 0.15135 | valid_logloss: 0.16276 |  0:08:08s\n",
      "epoch 289| loss: 0.16129 | train_logloss: 0.15238 | valid_logloss: 0.16357 |  0:08:10s\n",
      "epoch 290| loss: 0.16016 | train_logloss: 0.1511  | valid_logloss: 0.1624  |  0:08:12s\n",
      "epoch 291| loss: 0.16188 | train_logloss: 0.15079 | valid_logloss: 0.16088 |  0:08:13s\n",
      "epoch 292| loss: 0.1592  | train_logloss: 0.14929 | valid_logloss: 0.1601  |  0:08:15s\n",
      "epoch 293| loss: 0.15878 | train_logloss: 0.14932 | valid_logloss: 0.16005 |  0:08:17s\n",
      "epoch 294| loss: 0.15697 | train_logloss: 0.14889 | valid_logloss: 0.15972 |  0:08:18s\n",
      "epoch 295| loss: 0.15786 | train_logloss: 0.14903 | valid_logloss: 0.16009 |  0:08:20s\n",
      "epoch 296| loss: 0.15684 | train_logloss: 0.14861 | valid_logloss: 0.15955 |  0:08:22s\n",
      "epoch 297| loss: 0.15706 | train_logloss: 0.14838 | valid_logloss: 0.15916 |  0:08:23s\n",
      "epoch 298| loss: 0.15686 | train_logloss: 0.14866 | valid_logloss: 0.15939 |  0:08:25s\n",
      "epoch 299| loss: 0.15528 | train_logloss: 0.14843 | valid_logloss: 0.15892 |  0:08:27s\n",
      "epoch 300| loss: 0.15727 | train_logloss: 0.14822 | valid_logloss: 0.15885 |  0:08:29s\n",
      "epoch 301| loss: 0.1557  | train_logloss: 0.14819 | valid_logloss: 0.15893 |  0:08:30s\n",
      "epoch 302| loss: 0.1566  | train_logloss: 0.14827 | valid_logloss: 0.15869 |  0:08:32s\n",
      "epoch 303| loss: 0.15646 | train_logloss: 0.1485  | valid_logloss: 0.15871 |  0:08:34s\n",
      "epoch 304| loss: 0.15695 | train_logloss: 0.14808 | valid_logloss: 0.15867 |  0:08:35s\n",
      "epoch 305| loss: 0.15724 | train_logloss: 0.14815 | valid_logloss: 0.15879 |  0:08:37s\n",
      "epoch 306| loss: 0.15627 | train_logloss: 0.14817 | valid_logloss: 0.15846 |  0:08:39s\n",
      "epoch 307| loss: 0.15609 | train_logloss: 0.14859 | valid_logloss: 0.15916 |  0:08:40s\n",
      "epoch 308| loss: 0.15622 | train_logloss: 0.1481  | valid_logloss: 0.15839 |  0:08:42s\n",
      "epoch 309| loss: 0.15739 | train_logloss: 0.14843 | valid_logloss: 0.16042 |  0:08:44s\n",
      "epoch 310| loss: 0.15868 | train_logloss: 0.1499  | valid_logloss: 0.16103 |  0:08:46s\n",
      "epoch 311| loss: 0.1603  | train_logloss: 0.14976 | valid_logloss: 0.16041 |  0:08:47s\n",
      "epoch 312| loss: 0.1576  | train_logloss: 0.14891 | valid_logloss: 0.16263 |  0:08:49s\n",
      "epoch 313| loss: 0.15681 | train_logloss: 0.15016 | valid_logloss: 0.16363 |  0:08:51s\n",
      "epoch 314| loss: 0.15936 | train_logloss: 0.15333 | valid_logloss: 0.16686 |  0:08:52s\n",
      "epoch 315| loss: 0.16078 | train_logloss: 0.15102 | valid_logloss: 0.1645  |  0:08:54s\n",
      "epoch 316| loss: 0.16445 | train_logloss: 0.153   | valid_logloss: 0.16538 |  0:08:56s\n",
      "epoch 317| loss: 0.16763 | train_logloss: 0.15189 | valid_logloss: 0.16685 |  0:08:57s\n",
      "epoch 318| loss: 0.16165 | train_logloss: 0.15219 | valid_logloss: 0.16437 |  0:08:59s\n",
      "epoch 319| loss: 0.1646  | train_logloss: 0.15231 | valid_logloss: 0.16418 |  0:09:01s\n",
      "epoch 320| loss: 0.16621 | train_logloss: 0.15409 | valid_logloss: 0.16708 |  0:09:02s\n",
      "epoch 321| loss: 0.16103 | train_logloss: 0.15068 | valid_logloss: 0.16445 |  0:09:04s\n",
      "epoch 322| loss: 0.16287 | train_logloss: 0.15394 | valid_logloss: 0.16443 |  0:09:06s\n",
      "epoch 323| loss: 0.1624  | train_logloss: 0.15185 | valid_logloss: 0.16539 |  0:09:07s\n",
      "epoch 324| loss: 0.16201 | train_logloss: 0.15261 | valid_logloss: 0.16429 |  0:09:09s\n",
      "epoch 325| loss: 0.16113 | train_logloss: 0.15277 | valid_logloss: 0.16507 |  0:09:11s\n",
      "epoch 326| loss: 0.163   | train_logloss: 0.1509  | valid_logloss: 0.16361 |  0:09:12s\n",
      "epoch 327| loss: 0.16477 | train_logloss: 0.15135 | valid_logloss: 0.16424 |  0:09:14s\n",
      "epoch 328| loss: 0.16121 | train_logloss: 0.15111 | valid_logloss: 0.16236 |  0:09:16s\n",
      "epoch 329| loss: 0.16023 | train_logloss: 0.14975 | valid_logloss: 0.16215 |  0:09:17s\n",
      "epoch 330| loss: 0.15843 | train_logloss: 0.14894 | valid_logloss: 0.16093 |  0:09:19s\n",
      "epoch 331| loss: 0.15968 | train_logloss: 0.1492  | valid_logloss: 0.16127 |  0:09:21s\n",
      "epoch 332| loss: 0.16003 | train_logloss: 0.14876 | valid_logloss: 0.1603  |  0:09:22s\n",
      "epoch 333| loss: 0.15875 | train_logloss: 0.14878 | valid_logloss: 0.16131 |  0:09:24s\n",
      "epoch 334| loss: 0.15684 | train_logloss: 0.14869 | valid_logloss: 0.16067 |  0:09:26s\n",
      "epoch 335| loss: 0.1559  | train_logloss: 0.14849 | valid_logloss: 0.16096 |  0:09:28s\n",
      "epoch 336| loss: 0.15782 | train_logloss: 0.1485  | valid_logloss: 0.16066 |  0:09:29s\n",
      "epoch 337| loss: 0.15605 | train_logloss: 0.14791 | valid_logloss: 0.16002 |  0:09:31s\n",
      "epoch 338| loss: 0.15711 | train_logloss: 0.14774 | valid_logloss: 0.15977 |  0:09:33s\n",
      "epoch 339| loss: 0.15674 | train_logloss: 0.14777 | valid_logloss: 0.15976 |  0:09:34s\n",
      "epoch 340| loss: 0.15746 | train_logloss: 0.14797 | valid_logloss: 0.16009 |  0:09:36s\n",
      "epoch 341| loss: 0.15612 | train_logloss: 0.14797 | valid_logloss: 0.1601  |  0:09:38s\n",
      "epoch 342| loss: 0.15556 | train_logloss: 0.14788 | valid_logloss: 0.16027 |  0:09:39s\n",
      "epoch 343| loss: 0.15512 | train_logloss: 0.14773 | valid_logloss: 0.16063 |  0:09:41s\n",
      "epoch 344| loss: 0.15688 | train_logloss: 0.14776 | valid_logloss: 0.16055 |  0:09:43s\n",
      "epoch 345| loss: 0.15759 | train_logloss: 0.14809 | valid_logloss: 0.16071 |  0:09:45s\n",
      "epoch 346| loss: 0.1582  | train_logloss: 0.1484  | valid_logloss: 0.16129 |  0:09:46s\n",
      "epoch 347| loss: 0.15786 | train_logloss: 0.14787 | valid_logloss: 0.1609  |  0:09:48s\n",
      "epoch 348| loss: 0.15683 | train_logloss: 0.14782 | valid_logloss: 0.16045 |  0:09:50s\n",
      "epoch 349| loss: 0.15602 | train_logloss: 0.14772 | valid_logloss: 0.16026 |  0:09:51s\n",
      "epoch 350| loss: 0.15768 | train_logloss: 0.14925 | valid_logloss: 0.16211 |  0:09:53s\n",
      "epoch 351| loss: 0.15825 | train_logloss: 0.14957 | valid_logloss: 0.16359 |  0:09:55s\n",
      "epoch 352| loss: 0.15811 | train_logloss: 0.14815 | valid_logloss: 0.16025 |  0:09:56s\n",
      "epoch 353| loss: 0.15729 | train_logloss: 0.15031 | valid_logloss: 0.16251 |  0:09:58s\n",
      "epoch 354| loss: 0.15623 | train_logloss: 0.14981 | valid_logloss: 0.16215 |  0:10:00s\n",
      "epoch 355| loss: 0.15824 | train_logloss: 0.14999 | valid_logloss: 0.16259 |  0:10:01s\n",
      "epoch 356| loss: 0.15751 | train_logloss: 0.14887 | valid_logloss: 0.16049 |  0:10:03s\n",
      "epoch 357| loss: 0.16093 | train_logloss: 0.15055 | valid_logloss: 0.16289 |  0:10:05s\n",
      "epoch 358| loss: 0.16136 | train_logloss: 0.15181 | valid_logloss: 0.16548 |  0:10:07s\n",
      "epoch 359| loss: 0.16139 | train_logloss: 0.15116 | valid_logloss: 0.16565 |  0:10:08s\n",
      "epoch 360| loss: 0.15727 | train_logloss: 0.14828 | valid_logloss: 0.16186 |  0:10:10s\n",
      "epoch 361| loss: 0.15837 | train_logloss: 0.14941 | valid_logloss: 0.16277 |  0:10:12s\n",
      "epoch 362| loss: 0.161   | train_logloss: 0.14975 | valid_logloss: 0.16195 |  0:10:13s\n",
      "epoch 363| loss: 0.15835 | train_logloss: 0.15097 | valid_logloss: 0.16309 |  0:10:15s\n",
      "epoch 364| loss: 0.15811 | train_logloss: 0.14845 | valid_logloss: 0.16055 |  0:10:17s\n",
      "epoch 365| loss: 0.16113 | train_logloss: 0.14883 | valid_logloss: 0.16066 |  0:10:18s\n",
      "epoch 366| loss: 0.15976 | train_logloss: 0.15047 | valid_logloss: 0.16315 |  0:10:20s\n",
      "epoch 367| loss: 0.15937 | train_logloss: 0.14985 | valid_logloss: 0.16362 |  0:10:22s\n",
      "epoch 368| loss: 0.15998 | train_logloss: 0.14867 | valid_logloss: 0.16159 |  0:10:24s\n",
      "epoch 369| loss: 0.15812 | train_logloss: 0.14868 | valid_logloss: 0.16114 |  0:10:25s\n",
      "epoch 370| loss: 0.15695 | train_logloss: 0.14842 | valid_logloss: 0.16188 |  0:10:27s\n",
      "epoch 371| loss: 0.15681 | train_logloss: 0.14836 | valid_logloss: 0.16115 |  0:10:29s\n",
      "epoch 372| loss: 0.1546  | train_logloss: 0.14824 | valid_logloss: 0.16158 |  0:10:30s\n",
      "epoch 373| loss: 0.15773 | train_logloss: 0.14759 | valid_logloss: 0.16116 |  0:10:32s\n",
      "epoch 374| loss: 0.15511 | train_logloss: 0.14662 | valid_logloss: 0.16057 |  0:10:34s\n",
      "epoch 375| loss: 0.15385 | train_logloss: 0.14663 | valid_logloss: 0.16039 |  0:10:35s\n",
      "epoch 376| loss: 0.15287 | train_logloss: 0.14668 | valid_logloss: 0.16018 |  0:10:37s\n",
      "epoch 377| loss: 0.15354 | train_logloss: 0.14636 | valid_logloss: 0.15966 |  0:10:39s\n",
      "epoch 378| loss: 0.15416 | train_logloss: 0.14671 | valid_logloss: 0.15983 |  0:10:40s\n",
      "epoch 379| loss: 0.15341 | train_logloss: 0.14636 | valid_logloss: 0.15971 |  0:10:42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 380| loss: 0.15386 | train_logloss: 0.14632 | valid_logloss: 0.1598  |  0:10:44s\n",
      "epoch 381| loss: 0.15396 | train_logloss: 0.14652 | valid_logloss: 0.16016 |  0:10:45s\n",
      "epoch 382| loss: 0.15327 | train_logloss: 0.14627 | valid_logloss: 0.15982 |  0:10:47s\n",
      "epoch 383| loss: 0.1529  | train_logloss: 0.14602 | valid_logloss: 0.15956 |  0:10:49s\n",
      "epoch 384| loss: 0.15315 | train_logloss: 0.14573 | valid_logloss: 0.15953 |  0:10:50s\n",
      "epoch 385| loss: 0.15353 | train_logloss: 0.14605 | valid_logloss: 0.15973 |  0:10:52s\n",
      "epoch 386| loss: 0.15456 | train_logloss: 0.14597 | valid_logloss: 0.15996 |  0:10:54s\n",
      "epoch 387| loss: 0.15427 | train_logloss: 0.14612 | valid_logloss: 0.15965 |  0:10:55s\n",
      "epoch 388| loss: 0.15377 | train_logloss: 0.1463  | valid_logloss: 0.15761 |  0:10:57s\n",
      "epoch 389| loss: 0.1554  | train_logloss: 0.14763 | valid_logloss: 0.15934 |  0:10:59s\n",
      "epoch 390| loss: 0.15572 | train_logloss: 0.1468  | valid_logloss: 0.15807 |  0:11:01s\n",
      "epoch 391| loss: 0.15794 | train_logloss: 0.15041 | valid_logloss: 0.1613  |  0:11:02s\n",
      "epoch 392| loss: 0.15856 | train_logloss: 0.14881 | valid_logloss: 0.15998 |  0:11:04s\n",
      "epoch 393| loss: 0.15735 | train_logloss: 0.14731 | valid_logloss: 0.15776 |  0:11:06s\n",
      "epoch 394| loss: 0.15899 | train_logloss: 0.1482  | valid_logloss: 0.15781 |  0:11:07s\n",
      "epoch 395| loss: 0.15717 | train_logloss: 0.14943 | valid_logloss: 0.15974 |  0:11:09s\n",
      "epoch 396| loss: 0.16087 | train_logloss: 0.14842 | valid_logloss: 0.15951 |  0:11:11s\n",
      "epoch 397| loss: 0.15939 | train_logloss: 0.14942 | valid_logloss: 0.16053 |  0:11:12s\n",
      "epoch 398| loss: 0.16284 | train_logloss: 0.14977 | valid_logloss: 0.16114 |  0:11:14s\n",
      "epoch 399| loss: 0.16296 | train_logloss: 0.14901 | valid_logloss: 0.15961 |  0:11:16s\n",
      "epoch 400| loss: 0.15923 | train_logloss: 0.15166 | valid_logloss: 0.16179 |  0:11:17s\n",
      "epoch 401| loss: 0.15919 | train_logloss: 0.15263 | valid_logloss: 0.1628  |  0:11:19s\n",
      "epoch 402| loss: 0.1581  | train_logloss: 0.14913 | valid_logloss: 0.16101 |  0:11:21s\n",
      "epoch 403| loss: 0.15773 | train_logloss: 0.15111 | valid_logloss: 0.16548 |  0:11:22s\n",
      "epoch 404| loss: 0.16078 | train_logloss: 0.15259 | valid_logloss: 0.16553 |  0:11:24s\n",
      "epoch 405| loss: 0.15955 | train_logloss: 0.15042 | valid_logloss: 0.16647 |  0:11:26s\n",
      "epoch 406| loss: 0.16266 | train_logloss: 0.15138 | valid_logloss: 0.16744 |  0:11:27s\n",
      "epoch 407| loss: 0.15967 | train_logloss: 0.14933 | valid_logloss: 0.16561 |  0:11:29s\n",
      "epoch 408| loss: 0.15914 | train_logloss: 0.15036 | valid_logloss: 0.16551 |  0:11:31s\n",
      "epoch 409| loss: 0.1564  | train_logloss: 0.14758 | valid_logloss: 0.16241 |  0:11:33s\n",
      "epoch 410| loss: 0.15537 | train_logloss: 0.14782 | valid_logloss: 0.16234 |  0:11:34s\n",
      "epoch 411| loss: 0.15614 | train_logloss: 0.14732 | valid_logloss: 0.16187 |  0:11:36s\n",
      "epoch 412| loss: 0.15478 | train_logloss: 0.14612 | valid_logloss: 0.16059 |  0:11:38s\n",
      "epoch 413| loss: 0.15373 | train_logloss: 0.14578 | valid_logloss: 0.16076 |  0:11:39s\n",
      "epoch 414| loss: 0.15463 | train_logloss: 0.14612 | valid_logloss: 0.16082 |  0:11:41s\n",
      "epoch 415| loss: 0.15378 | train_logloss: 0.14578 | valid_logloss: 0.16102 |  0:11:43s\n",
      "epoch 416| loss: 0.15452 | train_logloss: 0.1457  | valid_logloss: 0.16075 |  0:11:44s\n",
      "epoch 417| loss: 0.15324 | train_logloss: 0.14564 | valid_logloss: 0.16098 |  0:11:46s\n",
      "epoch 418| loss: 0.15284 | train_logloss: 0.1456  | valid_logloss: 0.16085 |  0:11:48s\n",
      "epoch 419| loss: 0.15412 | train_logloss: 0.14569 | valid_logloss: 0.16079 |  0:11:50s\n",
      "epoch 420| loss: 0.15469 | train_logloss: 0.14586 | valid_logloss: 0.16084 |  0:11:51s\n",
      "epoch 421| loss: 0.15292 | train_logloss: 0.14576 | valid_logloss: 0.1609  |  0:11:53s\n",
      "epoch 422| loss: 0.15538 | train_logloss: 0.14592 | valid_logloss: 0.16092 |  0:11:55s\n",
      "epoch 423| loss: 0.15267 | train_logloss: 0.14587 | valid_logloss: 0.16094 |  0:11:56s\n",
      "epoch 424| loss: 0.1548  | train_logloss: 0.1457  | valid_logloss: 0.16086 |  0:11:58s\n",
      "epoch 425| loss: 0.15321 | train_logloss: 0.14549 | valid_logloss: 0.1609  |  0:12:00s\n",
      "epoch 426| loss: 0.15375 | train_logloss: 0.14549 | valid_logloss: 0.16051 |  0:12:01s\n",
      "epoch 427| loss: 0.15463 | train_logloss: 0.14604 | valid_logloss: 0.16031 |  0:12:03s\n",
      "epoch 428| loss: 0.15332 | train_logloss: 0.14639 | valid_logloss: 0.16062 |  0:12:05s\n",
      "epoch 429| loss: 0.1552  | train_logloss: 0.14692 | valid_logloss: 0.16146 |  0:12:07s\n",
      "epoch 430| loss: 0.15417 | train_logloss: 0.14767 | valid_logloss: 0.16329 |  0:12:08s\n",
      "epoch 431| loss: 0.15732 | train_logloss: 0.14643 | valid_logloss: 0.16476 |  0:12:10s\n",
      "epoch 432| loss: 0.15415 | train_logloss: 0.14733 | valid_logloss: 0.16598 |  0:12:12s\n",
      "epoch 433| loss: 0.1568  | train_logloss: 0.14827 | valid_logloss: 0.16631 |  0:12:13s\n",
      "epoch 434| loss: 0.15529 | train_logloss: 0.1497  | valid_logloss: 0.16437 |  0:12:15s\n",
      "epoch 435| loss: 0.15683 | train_logloss: 0.15218 | valid_logloss: 0.16913 |  0:12:17s\n",
      "epoch 436| loss: 0.16425 | train_logloss: 0.15155 | valid_logloss: 0.16626 |  0:12:18s\n",
      "epoch 437| loss: 0.16101 | train_logloss: 0.15001 | valid_logloss: 0.16278 |  0:12:20s\n",
      "epoch 438| loss: 0.15551 | train_logloss: 0.14893 | valid_logloss: 0.16144 |  0:12:22s\n",
      "epoch 439| loss: 0.16004 | train_logloss: 0.1471  | valid_logloss: 0.161   |  0:12:23s\n",
      "epoch 440| loss: 0.15904 | train_logloss: 0.14944 | valid_logloss: 0.16432 |  0:12:25s\n",
      "epoch 441| loss: 0.16292 | train_logloss: 0.14889 | valid_logloss: 0.16558 |  0:12:27s\n",
      "epoch 442| loss: 0.16025 | train_logloss: 0.1488  | valid_logloss: 0.16452 |  0:12:28s\n",
      "epoch 443| loss: 0.1571  | train_logloss: 0.14808 | valid_logloss: 0.16322 |  0:12:30s\n",
      "epoch 444| loss: 0.15629 | train_logloss: 0.14805 | valid_logloss: 0.16309 |  0:12:32s\n",
      "epoch 445| loss: 0.15673 | train_logloss: 0.14764 | valid_logloss: 0.16185 |  0:12:34s\n",
      "epoch 446| loss: 0.15929 | train_logloss: 0.15009 | valid_logloss: 0.1639  |  0:12:35s\n",
      "epoch 447| loss: 0.16079 | train_logloss: 0.15003 | valid_logloss: 0.16424 |  0:12:37s\n",
      "epoch 448| loss: 0.15625 | train_logloss: 0.14799 | valid_logloss: 0.16317 |  0:12:39s\n",
      "epoch 449| loss: 0.15655 | train_logloss: 0.14708 | valid_logloss: 0.16248 |  0:12:40s\n",
      "epoch 450| loss: 0.15557 | train_logloss: 0.14629 | valid_logloss: 0.16158 |  0:12:42s\n",
      "epoch 451| loss: 0.155   | train_logloss: 0.14661 | valid_logloss: 0.16126 |  0:12:44s\n",
      "epoch 452| loss: 0.15567 | train_logloss: 0.14645 | valid_logloss: 0.16099 |  0:12:45s\n",
      "epoch 453| loss: 0.15524 | train_logloss: 0.14631 | valid_logloss: 0.16108 |  0:12:47s\n",
      "epoch 454| loss: 0.1548  | train_logloss: 0.14588 | valid_logloss: 0.16099 |  0:12:49s\n",
      "epoch 455| loss: 0.15418 | train_logloss: 0.14577 | valid_logloss: 0.16077 |  0:12:51s\n",
      "epoch 456| loss: 0.15442 | train_logloss: 0.14546 | valid_logloss: 0.1603  |  0:12:52s\n",
      "epoch 457| loss: 0.1535  | train_logloss: 0.14542 | valid_logloss: 0.16013 |  0:12:54s\n",
      "epoch 458| loss: 0.15133 | train_logloss: 0.14543 | valid_logloss: 0.16011 |  0:12:56s\n",
      "epoch 459| loss: 0.15197 | train_logloss: 0.14556 | valid_logloss: 0.16029 |  0:12:57s\n",
      "epoch 460| loss: 0.15394 | train_logloss: 0.14558 | valid_logloss: 0.16031 |  0:12:59s\n",
      "epoch 461| loss: 0.15401 | train_logloss: 0.14567 | valid_logloss: 0.16047 |  0:13:01s\n",
      "epoch 462| loss: 0.15262 | train_logloss: 0.14585 | valid_logloss: 0.16064 |  0:13:02s\n",
      "epoch 463| loss: 0.15183 | train_logloss: 0.14558 | valid_logloss: 0.16055 |  0:13:04s\n",
      "epoch 464| loss: 0.15287 | train_logloss: 0.146   | valid_logloss: 0.16093 |  0:13:06s\n",
      "epoch 465| loss: 0.15259 | train_logloss: 0.14567 | valid_logloss: 0.16081 |  0:13:07s\n",
      "epoch 466| loss: 0.15257 | train_logloss: 0.14556 | valid_logloss: 0.16065 |  0:13:09s\n",
      "epoch 467| loss: 0.15266 | train_logloss: 0.14613 | valid_logloss: 0.16093 |  0:13:11s\n",
      "epoch 468| loss: 0.1537  | train_logloss: 0.14608 | valid_logloss: 0.16103 |  0:13:12s\n",
      "epoch 469| loss: 0.15349 | train_logloss: 0.14527 | valid_logloss: 0.15983 |  0:13:14s\n",
      "epoch 470| loss: 0.15424 | train_logloss: 0.14802 | valid_logloss: 0.1623  |  0:13:16s\n",
      "epoch 471| loss: 0.15447 | train_logloss: 0.14677 | valid_logloss: 0.16149 |  0:13:18s\n",
      "epoch 472| loss: 0.15572 | train_logloss: 0.14783 | valid_logloss: 0.16026 |  0:13:19s\n",
      "epoch 473| loss: 0.15551 | train_logloss: 0.14696 | valid_logloss: 0.15973 |  0:13:21s\n",
      "epoch 474| loss: 0.15495 | train_logloss: 0.1462  | valid_logloss: 0.15969 |  0:13:23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 475| loss: 0.15475 | train_logloss: 0.14611 | valid_logloss: 0.16056 |  0:13:24s\n",
      "epoch 476| loss: 0.15466 | train_logloss: 0.14549 | valid_logloss: 0.15978 |  0:13:26s\n",
      "epoch 477| loss: 0.15711 | train_logloss: 0.14657 | valid_logloss: 0.16164 |  0:13:28s\n",
      "epoch 478| loss: 0.15968 | train_logloss: 0.14652 | valid_logloss: 0.16276 |  0:13:29s\n",
      "epoch 479| loss: 0.1589  | train_logloss: 0.14859 | valid_logloss: 0.16314 |  0:13:31s\n",
      "epoch 480| loss: 0.15807 | train_logloss: 0.14802 | valid_logloss: 0.15948 |  0:13:33s\n",
      "epoch 481| loss: 0.15688 | train_logloss: 0.14634 | valid_logloss: 0.15944 |  0:13:34s\n",
      "epoch 482| loss: 0.15755 | train_logloss: 0.14723 | valid_logloss: 0.15936 |  0:13:36s\n",
      "epoch 483| loss: 0.15523 | train_logloss: 0.14538 | valid_logloss: 0.15847 |  0:13:38s\n",
      "epoch 484| loss: 0.15811 | train_logloss: 0.1461  | valid_logloss: 0.15962 |  0:13:40s\n",
      "epoch 485| loss: 0.15483 | train_logloss: 0.14503 | valid_logloss: 0.1597  |  0:13:41s\n",
      "epoch 486| loss: 0.15431 | train_logloss: 0.14524 | valid_logloss: 0.15928 |  0:13:43s\n",
      "epoch 487| loss: 0.15503 | train_logloss: 0.14526 | valid_logloss: 0.15917 |  0:13:45s\n",
      "epoch 488| loss: 0.15783 | train_logloss: 0.14608 | valid_logloss: 0.15989 |  0:13:46s\n",
      "\n",
      "Early stopping occurred at epoch 488 with best_epoch = 388 and best_valid_logloss = 0.15761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosal/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train.values, y_train=Y_train.values,  \n",
    "    eval_set=[(X_train.values, Y_train.values), (X_test.values, Y_test.values)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    max_epochs=2000,\n",
    "    patience=100,\n",
    "    batch_size=128,\n",
    "    virtual_batch_size=64,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    loss_fn=[torch.nn.functional.cross_entropy]*Y_train.shape[1],  \n",
    "    from_unsupervised=unsupervised_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "33292315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./MODEL/ClfModel_0.1576.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./MODEL/ClfModel_0.1576.zip'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.save_model('./MODEL/ClfModel_0.1576')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1983ac1",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7a2ba64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bac9f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(X_test.values)\n",
    "AUROCs = [roc_auc_score(Y_test.values[:,idx], res[idx][:,1]) for idx in range(Y_test.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "62a34ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8118014152783195,\n",
       " 0.7994980449013663,\n",
       " 0.7880316482011398,\n",
       " 0.7849857498029228,\n",
       " 0.9050495198079231,\n",
       " 0.9491614255765198,\n",
       " 1.0,\n",
       " 0.9999888118147237,\n",
       " 0.9958181660599758,\n",
       " 0.9990641711229946,\n",
       " 0.92191204362257,\n",
       " 0.9113363363363363,\n",
       " 0.9835679460390776,\n",
       " 0.9969939195190272,\n",
       " 0.9680563518715701,\n",
       " 0.9994828275229789,\n",
       " 0.9972013744901634,\n",
       " 0.999166301427292,\n",
       " 0.9988548709210503,\n",
       " 1.0,\n",
       " 0.9991616364855801,\n",
       " 0.9982722377400869,\n",
       " 0.9931412200876306,\n",
       " 0.8165197155545665,\n",
       " 0.9948226869743122,\n",
       " 0.8650846508754657,\n",
       " 0.9999406985708356,\n",
       " 0.9973229224762967]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "898f0c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 31)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8fcfd144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11061031, 0.01071639, 0.        , 0.        , 0.067394  ,\n",
       "       0.07172143, 0.02985679, 0.01490689, 0.02709706, 0.00676125,\n",
       "       0.        , 0.00171039, 0.03122171, 0.00174737, 0.02425583,\n",
       "       0.        , 0.05045785, 0.08060017, 0.1182835 , 0.03502563,\n",
       "       0.0035272 , 0.01536798, 0.11151161, 0.00495678, 0.00292322,\n",
       "       0.03191978, 0.00019516, 0.        , 0.09396729, 0.        ,\n",
       "       0.05326441])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
